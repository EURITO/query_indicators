{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clio_lite import clio_search, clio_search_iter\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import io\n",
    "\n",
    "plt.ioff()  # <--- for turning off visible figs\n",
    "\n",
    "mpl.rcParams['hatch.linewidth'] = 0.2\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['image.cmap'] = 'Pastel1'\n",
    "\n",
    "os.environ['AWS_SHARED_CREDENTIALS_FILE'] = '/Users/jklinger/EURITO-AWS/.aws/credentials'  # <--- Note: NOT nesta's AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eu_countries():\n",
    "    \"\"\"Extact all EU country codes\"\"\"\n",
    "    url = 'https://restcountries.eu/rest/v2/regionalbloc/eu'\n",
    "    r = requests.get(url)\n",
    "    return [row['alpha2Code'] for row in r.json()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some globals\n",
    "URL = \"https://search-eurito-dev-vq22tw6otqjpdh47u75bh2g7ba.eu-west-2.es.amazonaws.com/\"\n",
    "INDEX = \"arxiv_v0\" \n",
    "FIELDS = ['terms_tokens_entity', 'textBody_abstract_article']\n",
    "EU_COUNTRIES = get_eu_countries()\n",
    "COLORS = plt.get_cmap('Set2').colors\n",
    "S3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_search(query, max_query_terms, yr0=2014, yr1=2019, countries=EU_COUNTRIES, window=1):\n",
    "    \"\"\"\n",
    "    Retrieve count and score data for a given basic clio search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Seed query for clio.\n",
    "        max_query_terms (list): Triple of max_query_terms (low, middle, high) to use from the initial query.\n",
    "        yr0 (int): Start year in range to use in filter.\n",
    "        yr1 (int): Final year in range to use in filter.\n",
    "        countries (list): A list of countries to filter (default to all EU).\n",
    "        window (int): The number of years to consider in between time windows. Note that changing this will lead to double-counting.\n",
    "    Returns:\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each country]}\n",
    "        all_scores (dict): {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "    \"\"\"\n",
    "    top_doc = None\n",
    "    _data = defaultdict(lambda: defaultdict(dict))  # {max_query_terms --> {year --> {country --> score} } }\n",
    "    all_scores = defaultdict(lambda: defaultdict(list))  # {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "    for n in max_query_terms:\n",
    "        # Set the order of the countries\n",
    "        for ctry in EU_COUNTRIES:\n",
    "            _data[n][ctry]\n",
    "            all_scores[n][ctry]\n",
    "        # Iterate over years\n",
    "        for yr in range(yr0, yr1+1):\n",
    "            # Set default values for countries\n",
    "            for ctry in EU_COUNTRIES:\n",
    "                _data[n][ctry][yr] = 0            \n",
    "            # Iterate over docs\n",
    "            filters = [{\"range\":{\"year_of_article\":{\"gte\":yr, \"lt\":yr+window}}}]\n",
    "            for doc in clio_search_iter(url=URL, index=INDEX, query=query, fields=FIELDS,\n",
    "                                        max_query_terms=n, post_filters=filters, chunksize=5000):\n",
    "                if '_score' not in doc or doc['terms_countries_article'] is None:\n",
    "                    continue\n",
    "                if top_doc is None:\n",
    "                    top_doc = doc\n",
    "                score = doc['_score']\n",
    "                for ctry in filter(lambda x: x in countries, doc['terms_countries_article']):\n",
    "                    all_scores[n][ctry].append(score)\n",
    "                    _data[n][ctry][yr] += score\n",
    "    # Reformat data as {max_query_terms --> [{year --> score} for each country in order]}\n",
    "    data = {}\n",
    "    for n, ctry_data in _data.items():\n",
    "        data[n] = []\n",
    "        for ctry, yr_data in ctry_data.items():\n",
    "            data[n].append(yr_data)\n",
    "    return top_doc, data, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator calculations\n",
    "\n",
    "Each of these functions is assumed to take the form\n",
    "\n",
    "```python\n",
    "def _an_indicator_calulation(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    A function calculating an indicator.\n",
    "    \n",
    "    Args:\n",
    "        data (list): Rows of data\n",
    "        year (int): A year to consider, if applicable.\n",
    "        _max (int): Divide by this to normalise your results. This is automatically applied in :obj:`make_activity_plot`\n",
    "    Returns:\n",
    "        result (list) A list of indicators to plot. The length of the list is assumed to be equal to the number of countries.\n",
    "    \"\"\"\n",
    "    # Calculate something\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _total_activity_by_country(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Sum of relevance scores, by year (if specified) or in total.\n",
    "    \"\"\"    \n",
    "    if year is None:        \n",
    "        scores = [sum(row.values())/_max for row in data]\n",
    "    else:\n",
    "        scores = [row[year]/_max for row in data]\n",
    "    return scores\n",
    "      \n",
    "\n",
    "def _average_activity_by_country(data, year=None, _max=1):    \n",
    "    \"\"\"\n",
    "    Indicator: Mean relevance score. This function is basically a lambda, since it assumes the average has already been calculated.\n",
    "    \"\"\"        \n",
    "    return [row/_max for row in data]\n",
    "    \n",
    "    \n",
    "def _corrected_average_activity_by_country(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Mean relevance score minus it's (very) approximate Poisson error.\n",
    "    \"\"\"    \n",
    "    return [(row - np.sqrt(row))/_max for row in data]\n",
    "    \n",
    "\n",
    "def _linear_coeffs(years, scores, _max):\n",
    "    \"\"\"Calculates linear coefficients for scores wrt years\"\"\"\n",
    "    return [np.polyfit(_scores, _years, 1)[0]/_max\n",
    "            if all(v > 0 for v in _scores) else 0\n",
    "            for _years, _scores in zip(years, scores)]    \n",
    "    \n",
    "\n",
    "def _trajectory(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Linear coefficient of total relevance score wrt year\n",
    "    \"\"\"\n",
    "    years = [list(row.keys()) for row in data]\n",
    "    scores = [list(row.values()) for row in data]\n",
    "    return _linear_coeffs(years, scores, _max)\n",
    "\n",
    "\n",
    "def _corrected_trajectory(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Linear coefficient of upper and lower limits of relevance score wrt year\n",
    "    \"\"\" \n",
    "    # Reformulate the data in terms of upper and lower bounds\n",
    "    years, scores = [], []\n",
    "    for row in data:\n",
    "        _years, _scores = [], []\n",
    "        for k, v in row.items():\n",
    "            _years += [k,k]\n",
    "            _scores += [v - np.sqrt(v), v + np.sqrt(v)]  # Estimate upper and lower limits with very approximate Poisson errors\n",
    "        years.append(_years)\n",
    "        scores.append(_scores)\n",
    "    return _linear_coeffs(years, scores, _max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Sorter:\n",
    "    def __init__(self, values, topn=None):\n",
    "        if topn is None:\n",
    "            topn = len(values)\n",
    "        self.indices = list(np.argsort(values))[-topn:]  # Argsort is ascending, so -ve indexing to pick up topn\n",
    "    def sort(self, x):\n",
    "        \"\"\"Sort list x by indices\"\"\"\n",
    "        return [x[i] for i in self.indices]\n",
    "\n",
    "\n",
    "def _s3_savefig(bucket, indicator_family, query, fig_name, extension='png'):\n",
    "    \"\"\"Save the figure to s3. The figure is grabbed from the global scope.\"\"\"\n",
    "    outname = (f'figures/{indicator_family}/'\n",
    "               f'{query.replace(\" \",\"_\").lower()}'\n",
    "               f'/{fig_name.replace(\" \",\"_\").lower()}'\n",
    "               f'.{extension}')\n",
    "    with io.BytesIO() as f:\n",
    "        plt.savefig(f, bbox_inches='tight', format=extension, pad_inches=0)\n",
    "        obj = S3.Object(bucket, outname)\n",
    "        f.seek(0)\n",
    "        obj.put(Body=f)\n",
    "\n",
    "        \n",
    "def _s3_savetable(data, key, index, object_path, bucket='eurito-indicators', transformer=lambda x: x):\n",
    "    \"\"\"Upload the table to s3\"\"\"\n",
    "    df = pd.DataFrame(transformer(data[key]), index=index)\n",
    "    if len(df.columns) == 1:\n",
    "        df.columns = ['value']\n",
    "    df = df / df.max().max()\n",
    "    table_data = df.to_csv().encode()\n",
    "    obj = S3.Object(bucket, object_path)\n",
    "    obj.put(Body=table_data)\n",
    "\n",
    "        \n",
    "def make_activity_plot(f, data, countries, max_query_terms, query, \n",
    "                       bucket, indicator_family, year=None, label=None, \n",
    "                       x_padding=0.5, y_padding=0.05, xlabel_fontsize=14):\n",
    "    \"\"\"\n",
    "    Make a query and generate indicators by country, saving the plots to S3 and saving the rawest data\n",
    "    to tables on S3.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        f: An indicator function, as described in the 'Indicator calculations' section.\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each country]}\n",
    "        countries (list): A list of EU ISO-2 codes        \n",
    "        max_query_terms (list): Triple of max_query_terms for clio, corresponding to low, middle and high values of \n",
    "                                max_query_terms to test robustness of the query.\n",
    "        query (str): query used to generate this data.\n",
    "        bucket (str): The name of the s3 bucket to store your histogram.\n",
    "        indicator_family (str): The name of this indicator family, for labelling in S3 file system.\n",
    "        year (int): Year to generate the indicator for (if applicable).\n",
    "        label (str): label for annotating the plot.\n",
    "        {x,y}_padding (float): Aesthetic padding around the extreme limits of the {x,y} axis.\n",
    "        xlabel_fontsize (int): Fontsize of the x labels (country ISO-2 codes).\n",
    "    \"\"\"    \n",
    "    # Calculate the indicator for each value of n, then recalculate the normalised indicator\n",
    "    _, middle, _ = (f(data[n], year=year) for n in max_query_terms)\n",
    "    low, middle, high = (f(data[n], year=year, _max=max(middle)) for n in max_query_terms)\n",
    "    indicator = [np.median([a, b, c]) for a, b, c in zip(low, middle, high)]    \n",
    "\n",
    "    # Sort all data by indicator value\n",
    "    s = _Sorter(indicator)\n",
    "    countries = s.sort(countries)\n",
    "    low = s.sort(low)\n",
    "    middle = s.sort(middle)\n",
    "    high =  s.sort(high)\n",
    "    indicator = s.sort(indicator)\n",
    "\n",
    "    # Make the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))    \n",
    "    make_error_boxes(ax, low, middle, high)  # Draw the bounding box\n",
    "    ax.scatter(countries, indicator,  s=0, marker='o', color='black')  # Draw the centre mark\n",
    "    ax.set_title(f'{label}\\nQuery: \"{query}\"')\n",
    "    ax.set_ylabel(label)\n",
    "\n",
    "    # Set limits and formulate \n",
    "    y0 = min(low+middle+high)    \n",
    "    y1 = max(low+middle+high)\n",
    "    if -y1*y_padding < y0:\n",
    "        y0 = -y1*y_padding\n",
    "    else:  # In case of negative values\n",
    "        y0 = y0 - np.abs(y0*y_padding)\n",
    "    ax.set_ylim(y0, y1*(1+y_padding))\n",
    "    ax.set_xlim(-x_padding, len(countries)-x_padding)\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(xlabel_fontsize)\n",
    "    \n",
    "    # Save to s3 & return\n",
    "    _s3_savefig(bucket, indicator_family, query, label)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def make_error_boxes(ax, low, middle, high, facecolor='r',\n",
    "                     edgecolor='None', alpha=0.5):\n",
    "    \"\"\"\n",
    "    Generate outer rectangles based on three values, and draw a horizontal line through the middle of the rectangle.\n",
    "    No assumption is made on the order of values, so don't worry if they're not properly ordered.\n",
    "        \n",
    "    Args:\n",
    "        ax (matplotlib.axis): An axis to add patches to.\n",
    "        {low, middle, high} (list): Three concurrent lists of values from which to calculate the rectangle limits.\n",
    "        {facecolor, edgecolor} (str): The {face,edge} colour of the rectangles.\n",
    "        alpha (float): The alpha of the rectangles.\n",
    "    \"\"\"\n",
    "    # Generate the rectangle\n",
    "    errorboxes = []\n",
    "    middlelines = []\n",
    "    for x, ys in enumerate(zip(low, middle, high)):        \n",
    "        rect = Rectangle((x - 0.45, min(ys)), 0.9, max(ys) - min(ys))\n",
    "        line = Rectangle((x - 0.45, np.median(ys)), 0.9, 0)\n",
    "        errorboxes.append(rect)\n",
    "        middlelines.append(line)\n",
    "\n",
    "    # Create patch collection with specified colour/alpha\n",
    "    pc = PatchCollection(errorboxes, facecolor=facecolor, alpha=alpha, edgecolor=edgecolor, hatch='/')\n",
    "    lc = PatchCollection(middlelines, facecolor='black', alpha=0.9, edgecolor='black')\n",
    "\n",
    "    # Add collection to axes\n",
    "    ax.add_collection(pc)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "\n",
    "def stacked_scores(all_scores, query, bucket, \n",
    "                   indicator_family, topn=8,\n",
    "                   low_bins=[10**i for i in np.arange(0, 1.1, 0.025)],\n",
    "                   high_bins=[10**i for i in np.arange(1.1, 2.5, 0.05)],\n",
    "                   x_scale='log', label='Relevance score breakdown', \n",
    "                   xlabel='Relevance score', ylabel='Number of relevant documents',\n",
    "                   legend_fontsize='small', legend_cols=2):\n",
    "    \"\"\"\n",
    "    Create stacked histogram of document scores by country. Two sets of bins are used, \n",
    "    in order to have a more legible binning scale.\n",
    "    \n",
    "    Args:\n",
    "        all_scores (dict): {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "        query (str): query used to generate this data.\n",
    "        bucket (str): The name of the s3 bucket to store your histogram.\n",
    "        indicator_family (str): The name of the s3 sub-bucket to store your histogram\n",
    "        low_bins (list): List of initial bin edges.\n",
    "        high_bins (list): List of supplementary bin edges. These could have a different spacing scheme to the lower bin edges.\n",
    "        x_scale (str): Argument for `ax.set_xscale`.\n",
    "        label (str): label for annotating the plot.\n",
    "        {x,y}_label (str): Argument for `ax.set_{x,y}label`.\n",
    "        legend_fontsize (str): Argument for legend fontsize.\n",
    "        legend_cols (str): Argument for legend ncol.        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort countries and scores by the sum of scores by country\n",
    "    countries = list(all_scores.keys())\n",
    "    scores = list(all_scores.values())    \n",
    "    s = _Sorter([sum(v) for v in scores], topn=topn)\n",
    "    scores = s.sort(scores)\n",
    "    countries = s.sort(countries)\n",
    "\n",
    "    # Plot the stacked scores\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.set_cmap('Pastel1')\n",
    "    ax.hist(scores, bins=low_bins+high_bins, stacked=True,\n",
    "            label=countries, color=COLORS[:len(scores)])\n",
    "    \n",
    "    # Prettify the plot\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(fontsize=legend_fontsize, ncol=legend_cols)\n",
    "    ax.set_xlim(low_bins[0], None)\n",
    "    ax.set_xscale(x_scale)\n",
    "    ax.set_title(f'{label}\\nQuery: \"{query}\"')\n",
    "    \n",
    "    # Save to s3\n",
    "    _s3_savefig(bucket, indicator_family, query, label)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indicator(q, indicator_family, max_query_terms=[7, 10, 13], \n",
    "                       bucket='eurito-indicators', countries=EU_COUNTRIES, \n",
    "                       *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Make a query and generate indicators by country, saving the plots to S3 and saving the rawest data\n",
    "    to tables on S3.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        q (str): The query to Elasticsearch\n",
    "        indicator_family (str): The name of this indicator family, for labelling in S3 file system.\n",
    "        max_query_terms (list): Triple of max_query_terms for clio, corresponding to low, middle and high values of \n",
    "                                max_query_terms to test robustness of the query.\n",
    "        countries (list): A list of EU ISO-2 codes\n",
    "    Returns:\n",
    "        top_doc (dict): The highest ranking document from the search.\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each country]}\n",
    "        all_scores (dict): {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make the search and retrieve scores by country, and the highest ranking doc\n",
    "    example_doc, data, all_scores = make_search(q, max_query_terms=max_query_terms, countries=countries, *args, **kwargs)\n",
    "\n",
    "    # Reformat the scores to calculate the average\n",
    "    avg_scores = defaultdict(list)\n",
    "    for ctry in countries:\n",
    "        for n, _scores in all_scores.items():\n",
    "            mean = np.mean(_scores[ctry]) if len(_scores[ctry]) > 0 else 0\n",
    "            avg_scores[n].append(mean)\n",
    "    \n",
    "    plot_kwargs = dict(countries=countries, max_query_terms=max_query_terms, \n",
    "                       bucket=bucket, indicator_family=indicator_family, query=q)\n",
    "    # Calculate loads of indicators and save the plots\n",
    "    _ = make_activity_plot(_total_activity_by_country, data, label='Total relevance score', **plot_kwargs)\n",
    "    _ = make_activity_plot(_average_activity_by_country, avg_scores, label='Average relevance', **plot_kwargs)\n",
    "    _ = make_activity_plot(_corrected_average_activity_by_country, avg_scores, label='Corrected average relevance',  **plot_kwargs)\n",
    "    _ = make_activity_plot(_trajectory, data, label='Trajectory', **plot_kwargs)\n",
    "    _ = make_activity_plot(_corrected_trajectory, data, label='Corrected trajectory', **plot_kwargs)\n",
    "    _ = stacked_scores(all_scores[max_query_terms[1]], query=q, bucket=bucket, indicator_family=indicator_family)\n",
    "    \n",
    "    # Save the basic raw data as tables. Note: not as rich as the plotted data.\n",
    "    prefix = f'tables/ai_indicators/{q.replace(\" \",\"_\").lower()}'\n",
    "    _s3_savetable(data, max_query_terms[1], index=countries, object_path=f'{prefix}/total_relevance.csv')\n",
    "    _s3_savetable(avg_scores, max_query_terms[1], index=countries, object_path=f'{prefix}/avg_relevance.csv')\n",
    "    _s3_savetable(data, max_query_terms[1], transformer=_trajectory, index=countries, \n",
    "                  object_path=f'{prefix}/trajectory.csv')\n",
    "    \n",
    "    plt.close('all')  # Clean up the memory cache (unbelievable that matplotlib doesn't do this)\n",
    "    return example_doc, data, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Generation\n",
      "{'_id': '1404.7296', '_index': 'arxiv_v0', 'booleanFlag_eu_article': True, 'booleanFlag_multinational_article': False, 'count_citations_article': 14, 'date_created_article': '2014-04-29', 'id_digitalObjectIdentifier_article': None, 'json_fieldsOfStudy_article': {'nodes': [['Computer science'], ['Machine learning', 'Artificial intelligence', 'Natural language processing'], ['Semantics', 'Parsing'], ['S-attributed grammar', 'Top-down parsing'], ['Bottom-up parsing'], ['Top-down parsing language']], 'links': [[[0, 0], [1, 0]], [[0, 0], [1, 1]], [[0, 0], [1, 2]], [[1, 1], [2, 0]], [[1, 2], [2, 1]], [[2, 1], [3, 0]], [[2, 1], [3, 1]], [[3, 1], [4, 0]], [[4, 0], [5, 0]]]}, 'terms_authors_article': ['Edward Grefenstette', 'Phil Blunsom', 'Karl Moritz Hermann', 'Nando De Freitas'], 'terms_category_article': ['Computation and Language'], 'terms_countries_article': ['CA', 'GB'], 'terms_fieldsOfStudy_article': ['Artificial intelligence', 'Machine learning', 'Computer science', 'Semantics', 'Bottom-up parsing', 'Top-down parsing', 'Natural language processing', 'Top-down parsing language', 'Parsing', 'S-attributed grammar'], 'terms_institutes_article': ['University Of British Columbia', 'University Of Oxford'], 'terms_regions_article': ['Northern Europe', 'Northern America'], 'terms_tokens_entity': ['make use of', 'deep learning', 'two', 'generation', 'syntactically', 'makes it', 'text', 'models', 'statistical', 'provides', 'system', 'suitable', 'tweets', 'permitting', 'atypical', 'semantic', 'representations', 'parses', 'analysis', 'without', 'questions', 'semantics', 'neural', 'syntactic', 'especially', 'development', 'paper', 'languages', 'parsers', 'queries', 'as well as', 'statements', 'distributional', 'allows', 'natural language', 'approaches', 'on top of', 'need', 'presents', 'build', 'novel', 'architecture', 'many', 'the union', 'grammatically', 'match', 'resource poor', 'malformed', 'ontology specific', 'parsing', 'successful', 'language'], 'textBody_abstract_article': 'Many successful approaches to semantic parsing build on top of the syntactic\\nanalysis of text, and make use of distributional representations or statistical\\nmodels to match parses to ontology-specific queries. This paper presents a\\nnovel deep learning architecture which provides a semantic parsing system\\nthrough the union of two neural models of language semantics. It allows for the\\ngeneration of ontology-specific queries from natural language statements and\\nquestions without the need for parsing, which makes it especially suitable to\\ngrammatically malformed or syntactically atypical text, such as tweets, as well\\nas permitting the development of semantic parsers for resource-poor languages.', 'title_of_article': 'A Deep Architecture for Semantic Parsing', 'type_of_entity': 'article', 'year_of_article': 2014, '_score': 127.92833}\n",
      "\n",
      "Speech recognition\n",
      "{'_id': '1407.1165', '_index': 'arxiv_v0', 'booleanFlag_eu_article': False, 'booleanFlag_multinational_article': False, 'count_citations_article': 13, 'date_created_article': '2014-07-04', 'id_digitalObjectIdentifier_article': None, 'json_fieldsOfStudy_article': {'nodes': [['Computer science'], ['Speech recognition', 'Artificial intelligence', 'Pattern recognition'], ['Utterance', 'Automatic speech', 'Vocabulary', 'Principal component analysis']], 'links': [[[0, 0], [1, 0]], [[0, 0], [1, 1]], [[0, 0], [1, 2]], [[1, 0], [2, 0]], [[1, 0], [2, 1]], [[1, 1], [2, 2]], [[1, 1], [2, 3]], [[1, 2], [2, 3]]]}, 'terms_authors_article': ['Amarsinh B Varpe', 'Ramesh R Manza', 'Pravin Yannawar', 'Prashant L Borde'], 'terms_category_article': ['Computer Vision and Pattern Recognition', 'Computation and Language'], 'terms_countries_article': ['IN'], 'terms_fieldsOfStudy_article': ['Computer science', 'Artificial intelligence', 'Principal component analysis', 'Automatic speech', 'Utterance', 'Vocabulary', 'Speech recognition', 'Pattern recognition'], 'terms_institutes_article': ['Dr. Babasaheb Ambedkar Marathwada University'], 'terms_regions_article': ['Southern Asia'], 'terms_tokens_entity': ['isolated', 'collection', 'normalized', 'automatic', 'zernike', 'contribute', 'feature', 'moments', 'words', 'using', 'component', 'conditions', 'audio', 'principal', 'area', 'contains', 'topic', 'reading', 'mel', 'attractive', 'mfcc', 'year', 'recognition', 'improve', 'machine', 'processing', 'computed', 'in order to', 'advances', 'space', 'features', 'results', 'paper', 'asr', 'frequency', 'utterance', 'researchers', 'standard', 'visual', 'dataset', 'recognize', 'cepstral', 'research', 'speech', 'domain', 'vocabulary', 'based on', 'attracted', 'word', 'objective', 'speakers', 'system', 'audio visual', 'names', 'speech recognition', 'many', 'vviswa', 'accuracy', 'city', 'pca', 'respectively', 'coefficients', 'independent', 'dimension', 'performance', 'recent', 'set', 'analysis', 'noisy', 'reduced', 'inclusion', 'signal'], 'textBody_abstract_article': 'Automatic Speech Recognition (ASR) by machine is an attractive research topic\\nin signal processing domain and has attracted many researchers to contribute in\\nthis area. In recent year, there have been many advances in automatic speech\\nreading system with the inclusion of audio and visual speech features to\\nrecognize words under noisy conditions. The objective of audio-visual speech\\nrecognition system is to improve recognition accuracy. In this paper we\\ncomputed visual features using Zernike moments and audio feature using Mel\\nFrequency Cepstral Coefficients (MFCC) on vVISWa (Visual Vocabulary of\\nIndependent Standard Words) dataset which contains collection of isolated set\\nof city names of 10 speakers. The visual features were normalized and dimension\\nof features set was reduced by Principal Component Analysis (PCA) in order to\\nrecognize the isolated word utterance on PCA space.The performance of\\nrecognition of isolated words based on visual only and audio only features\\nresults in 63.88 and 100 respectively.', 'title_of_article': 'Recognition of Isolated Words using Zernike and MFCC features for Audio\\n  Visual Speech Recognition', 'type_of_entity': 'article', 'year_of_article': 2014, '_score': 310.16165}\n",
      "\n",
      "Virtual Agents\n",
      "{'_id': '1402.5045', '_index': 'arxiv_v0', 'booleanFlag_eu_article': True, 'booleanFlag_multinational_article': False, 'count_citations_article': 1, 'date_created_article': '2014-02-20', 'id_digitalObjectIdentifier_article': None, 'json_fieldsOfStudy_article': {'nodes': [['Computer science'], ['Human–computer interaction', 'Knowledge management']], 'links': [[[0, 0], [1, 0]], [[0, 0], [1, 1]]]}, 'terms_authors_article': ['Nicolas Sabouret', 'Mathieu Chollet', 'Hazael Jones', 'Magalie Ochs', 'Catherine Pelachaud'], 'terms_category_article': ['Artificial Intelligence', 'Human-Computer Interaction', 'Computers and Society'], 'terms_countries_article': ['FR'], 'terms_fieldsOfStudy_article': ['Human–computer interaction', 'Computer science', 'Knowledge management'], 'terms_institutes_article': ['Télécom Paristech'], 'terms_regions_article': ['Western Europe'], 'terms_tokens_entity': ['propose', 'approach', 'indeed', 'appropriate', 'attitude', 'increased', 'expressed', 'enables', 'audiovisual', 'occur', 'empirical', 'use', 'corpus', 'express', 'model', 'used to', 'behaviour', 'train', 'interaction', 'mood', 'theoretical', 'develop', 'agent', 'non verbal', 'decade', 'social', 'post hoc', 'human', 'proposed', 'methodology', 'last', 'job interview', 'in order to', 'moreover', 'simulation', 'literature', 'emotions', 'analysis', 'also', 'job interviews', 'reason', 'social sciences', 'real life', 'context', 'be able to', 'attitudes', 'based', 'given', 'coaching', 'agents', 'user', 'course', 'different', 'virtual', 'paper', 'recruiters', 'personality', 'rapidly', 'display', 'interviews', 'situations', 'developed', 'combined'], 'textBody_abstract_article': 'The use of virtual agents in social coaching has increased rapidly in the\\nlast decade. In order to train the user in different situations than can occur\\nin real life, the virtual agent should be able to express different social\\nattitudes. In this paper, we propose a model of social attitudes that enables a\\nvirtual agent to reason on the appropriate social attitude to express during\\nthe interaction with a user given the course of the interaction, but also the\\nemotions, mood and personality of the agent. Moreover, the model enables the\\nvirtual agent to display its social attitude through its non-verbal behaviour.\\nThe proposed model has been developed in the context of job interview\\nsimulation. The methodology used to develop such a model combined a theoretical\\nand an empirical approach. Indeed, the model is based both on the literature in\\nHuman and Social Sciences on social attitudes but also on the analysis of an\\naudiovisual corpus of job interviews and on post-hoc interviews with the\\nrecruiters on their expressed attitudes during the job interview.', 'title_of_article': 'Expressing social attitudes in virtual agents for social training games', 'type_of_entity': 'article', 'year_of_article': 2014, '_score': 150.81497}\n",
      "\n",
      "Machine Learning Platforms\n",
      "{'_id': '1402.6013', '_index': 'arxiv_v0', 'booleanFlag_eu_article': True, 'booleanFlag_multinational_article': False, 'count_citations_article': 2, 'date_created_article': '2014-02-24', 'id_digitalObjectIdentifier_article': None, 'json_fieldsOfStudy_article': {'nodes': [['Computer science'], ['Data science', 'Machine learning', 'Data mining', 'Artificial intelligence']], 'links': [[[0, 0], [1, 0]], [[0, 0], [1, 1]], [[0, 0], [1, 2]], [[0, 0], [1, 3]]]}, 'terms_authors_article': ['Mikio L Braun', 'Cheng Soon Ong', 'Joaquin Vanschoren'], 'terms_category_article': ['Machine Learning', 'Digital Libraries'], 'terms_countries_article': ['NL', 'DE', 'AU'], 'terms_fieldsOfStudy_article': ['Artificial intelligence', 'Data mining', 'Data science', 'Computer science', 'Machine learning'], 'terms_institutes_article': ['University Of Melbourne', 'Leiden University', 'Technical University Of Berlin'], 'terms_regions_article': ['Western Europe', 'Australia and New Zealand'], 'terms_tokens_entity': ['experiments', 'openml', 'present', 'science', 'software', 'obtained', 'software packages', 'compare', 'repositories', 'researchers', 'go beyond', 'data sets', 'also', 'easily', 'platforms', 'traditional', 'others', 'allow', 'results', 'application', 'data', 'encourage', 'study', 'provides', 'access', 'mldata', 'machine learning', 'open', 'easy', 'solutions', 'share'], 'textBody_abstract_article': 'We present OpenML and mldata, open science platforms that provides easy\\naccess to machine learning data, software and results to encourage further\\nstudy and application. They go beyond the more traditional repositories for\\ndata sets and software packages in that they allow researchers to also easily\\nshare the results they obtained in experiments and to compare their solutions\\nwith those of others.', 'title_of_article': 'Open science in machine learning', 'type_of_entity': 'article', 'year_of_article': 2014, '_score': 79.9389}\n",
      "\n",
      "AI-Optimized Hardware\n",
      "{'_id': '1407.0051', '_index': 'arxiv_v0', 'booleanFlag_eu_article': True, 'booleanFlag_multinational_article': False, 'count_citations_article': 1, 'date_created_article': '2014-06-30', 'id_digitalObjectIdentifier_article': None, 'json_fieldsOfStudy_article': {'nodes': [['Computer science'], ['Simulation', 'Artificial intelligence'], ['Robot'], ['Robotics', 'Mobile robot']], 'links': [[[0, 0], [1, 0]], [[0, 0], [1, 1]], [[1, 1], [2, 0]], [[2, 0], [3, 0]], [[2, 0], [3, 1]]]}, 'terms_authors_article': ['Marte A Ramirezortegon', 'Daniel Zaldivar', 'Marco Perezcisneros', 'Erik Cuevas'], 'terms_category_article': ['Robotics'], 'terms_countries_article': ['DE', 'MX'], 'terms_fieldsOfStudy_article': ['Simulation', 'Mobile robot', 'Artificial intelligence', 'Computer science', 'Robot', 'Robotics'], 'terms_institutes_article': ['University Of Guadalajara', 'Humboldt University Of Berlin'], 'terms_regions_article': ['Western Europe', 'Latin America and the Caribbean'], 'terms_tokens_entity': ['possible', 'lack', 'wide', 'concepts', 'mathematical', 'learn', 'techniques', 'educating', 'algorithm', 'students', 'formal', 'value', 'goal', 'useful', 'use', 'action', 'years', 'laboratory', 'aide', 'many', 'important', 'fitness', 'using', 'several', 'guided', 'emerged', 'learning', 'material', 'disciplines', 'presents', 'paper', 'to draw', 'group', 'environment', 'ai', 'curriculum', 'autonomous', 'tools', 'presented', 'platform', 'experiments', 'response', 'university', 'becomes', 'methods', 'nontrivial', 'part', 'influenced', 'involving', 'teaching', 'by means of', 'engineers', 'context', 'la a', 'general', 'artificial intelligence', 'issues', 'la', 'solving', 'robotics', 'study', 'automata', 'hands on', 'exercises', 'robotic', 'motivates', 'simple', 'robot', 'however', 'procedure', 'assist', 'given', 'engineering', 'better', 'motivating', 'increases', 'convenient', 'including', 'traditional', 'computer science', 'various', 'offering', 'range', 'successfully', 'single', 'next', 'evaluated', 'addresses', 'building', 'handle', 'used', 'tested', 'enrolment', 'recent', 'holds', 'applied', 'overlap', 'course', 'approaches', 'central', 'reached', 'commonly', 'computer', 'proposal', 'subjects', 'determining', 'semester', 'methodology', 'guadalajara', 'retention', 'select', 'together', 'areas', 'directly', 'task', 'mobile', 'topics', 'basic', 'unifying', 'problems', 'robots'], 'textBody_abstract_article': 'In recent years, Artificial Intelligence techniques have emerged as useful\\ntools for solving various engineering problems that were not possible or\\nconvenient to handle by traditional methods. AI has directly influenced many\\nareas of computer science and becomes an important part of the engineering\\ncurriculum. However, determining the important topics for a single semester AI\\ncourse is a nontrivial task, given the lack of a general methodology. AI\\nconcepts commonly overlap with many other disciplines involving a wide range of\\nsubjects, including applied approaches to more formal mathematical issues. This\\npaper presents the use of a simple robotic platform to assist the learning of\\nbasic AI concepts. The study is guided through some simple experiments using\\nautonomous mobile robots. The central algorithm is the Learning Automata. Using\\nLA, each robot action is applied to an environment to be evaluated by means of\\na fitness value. The response of the environment is used by the automata to\\nselect its next action. This procedure holds until the goal task is reached.\\nThe proposal addresses the AI study by offering in LA a unifying context to\\ndraw together several of the topics of AI and motivating the students to learn\\nby building some hands on laboratory exercises. The presented material has been\\nsuccessfully tested as AI teaching aide in the University of Guadalajara\\nrobotics group as it motivates students and increases enrolment and retention\\nwhile educating better computer engineers.', 'title_of_article': 'Hands-on experiments on intelligent behavior for mobile robots', 'type_of_entity': 'article', 'year_of_article': 2014, '_score': 137.9142}\n",
      "\n",
      "Decision Management\n",
      "{'_id': '1409.2634', '_index': 'arxiv_v0', 'booleanFlag_eu_article': True, 'booleanFlag_multinational_article': False, 'count_citations_article': 4, 'date_created_article': '2014-09-09', 'id_digitalObjectIdentifier_article': '10.1007/978-3-642-28714-5_8', 'json_fieldsOfStudy_article': {'nodes': [['Engineering'], ['Systems engineering'], [], [], ['Business requirements', 'Requirement', 'Requirements management'], ['Requirement prioritization', 'Requirements traceability', 'Market requirements document']], 'links': [[[0, 0], [1, 0]], [[4, 0], [5, 0]], [[4, 1], [5, 0]], [[4, 1], [5, 1]], [[4, 1], [5, 2]], [[4, 2], [5, 0]]]}, 'terms_authors_article': ['Martin Jansen', 'Joachim Axmann', 'Tim Gulke', 'Bernhard Rumpe'], 'terms_category_article': ['Software Engineering'], 'terms_countries_article': ['CZ', 'DE', 'IT', 'IN', 'US'], 'terms_fieldsOfStudy_article': ['Engineering', 'Requirement prioritization', 'Requirements traceability', 'Systems engineering', 'Market requirements document', 'Business requirements', 'Requirements management', 'Requirement'], 'terms_institutes_article': ['Volkswagen (United States)', 'Volkswagen (India)', 'Volkswagen (Czechia)', 'Volkswagen (Italy)', 'Rwth Aachen University'], 'terms_regions_article': ['Northern America', 'Southern Asia', 'Eastern Europe', 'Western Europe', 'Southern Europe'], 'terms_tokens_entity': ['findings', 'exceed', 'may well', 'aachen', 'rwth', 'principal', 'lead', 'overhead', 'effective', 'plays', 'management', 'parts', 'connecting', 'ideas', 'detailed', 'process', 'automotive industry', 'problem', 'due to', 'project', 'gains', 'introducing', 'contribution', 'may', 'increased', 'however', 'decision making', \"today's\", 'market', 'direct', 'examination', 'research', 'effects', 'university', 'support', 'teams', 'positioning', 'development', 'changing', 'complexity', 'joined', 'role', 'become', 'product', 'profits', 'respect', 'present', 'volkswagen', 'expected', 'features', 'valuable', 'question', 'comes to', 'impact', 'described', 'vice versa', 'problem statement', 'tools', 'raised', 'important', 'could', 'practices', 'as well as', 'changed', 'oem', 'requirements', 'precise', 'cars', 'assist', 'results', 'characteristics', 'based on', 'of new', 'costs'], 'textBody_abstract_article': \"Effective requirements management plays an important role when it comes to\\nthe support of product development teams in the automotive industry. A precise\\npositioning of new cars in the market is based on features and characteristics\\ndescribed as requirements as well as on costs and profits. [Question/problem]\\nHowever, introducing or changing requirements does not only impact the product\\nand its parts, but may lead to overhead costs in the OEM due to increased\\ncomplexity. The raised overhead costs may well exceed expected gains or costs\\nfrom the changed requirements. [Principal ideas/results] By connecting\\nrequirements with direct and overhead costs, decision making based on\\nrequirements could become more valuable. [Contribution] This problem statement\\nresults from a detailed examination of the effects of requirements management\\npractices on process complexity and vice versa as well as on how today's\\nrequirements management tools assist in this respect. We present findings from\\na joined research project of RWTH Aachen University and Volkswagen\", 'title_of_article': 'High-Level Requirements Management and Complexity Costs in Automotive\\n  Development Projects: A Problem Statement', 'type_of_entity': 'article', 'year_of_article': 2014, '_score': 125.87326}\n",
      "\n",
      "Deep Learning Platforms\n",
      "{'_id': '1408.5093', '_index': 'arxiv_v0', 'booleanFlag_eu_article': True, 'booleanFlag_multinational_article': False, 'count_citations_article': 5360, 'date_created_article': '2014-06-20', 'id_digitalObjectIdentifier_article': None, 'json_fieldsOfStudy_article': {'nodes': [['Computer science'], ['Machine learning', 'Artificial intelligence', 'Computer vision'], [], ['Deep learning', 'Python (programming language)'], ['Theano']], 'links': [[[0, 0], [1, 0]], [[0, 0], [1, 1]], [[0, 0], [1, 2]], [[3, 0], [4, 0]], [[3, 1], [4, 0]]]}, 'terms_authors_article': ['Sergey Karayev', 'Ross B Girshick', 'Trevor Darrell', 'Jonathan Long', 'Sergio Guadarrama', 'Yangqing Jia', 'Jeff Donahue', 'Evan Shelhamer'], 'terms_category_article': ['Machine Learning', 'Neural and Evolutionary Computing', 'Computer Vision and Pattern Recognition'], 'terms_countries_article': ['US', 'CH', 'CA', 'IE', 'GB'], 'terms_fieldsOfStudy_article': ['Theano', 'Artificial intelligence', 'Computer vision', 'Computer science', 'Python (programming language)', 'Deep learning', 'Machine learning'], 'terms_institutes_article': ['Google (Ireland)', 'Google (Switzerland)', 'University Of California, Berkeley', 'Google (Canada)', 'Google (United Kingdom)', 'Google (United States)'], 'terms_regions_article': ['Western Europe', 'Northern America', 'Northern Europe'], 'terms_tokens_entity': ['machines', 'million', 'separating', 'multimedia', 'commodity', 'needs', 'fits', 'maintained', 'architectures', 'approx', 'developed', 'berkeley', 'industrial', 'practitioners', 'center', 'images', 'experimentation', 'collection', 'single', 'prototypes', 'large scale', 'platforms', 'startup', 'cuda', 'ms', 'powers', 'gpu', 'github', 'art', 'scientists', 'actual', 'caffe', 'implementation', 'deploying', 'deep', 'environments', 'image', 'development', 'framework', 'help', 'processing', 'library', 'deep learning', 'research', 'reference', 'speech', 'algorithms', 'model', 'industry', 'titan', 'provides', 'state of the', 'training', 'per', 'clean', 'bindings', 'internet scale', 'cloud', 'contributors', 'seamless', 'day', 'computation', 'python', 'ease', 'community', 'among', 'ongoing', 'vision', 'bsd licensed', 'active', 'allows', 'learning', 'c', 'media', 'projects', 'applications', 'models', 'bvlc', 'modifiable', 'deployment', 'efficiently', 'representation', 'matlab', 'convolutional neural networks', 'general purpose', 'switching', 'prototyping'], 'textBody_abstract_article': 'Caffe provides multimedia scientists and practitioners with a clean and\\nmodifiable framework for state-of-the-art deep learning algorithms and a\\ncollection of reference models. The framework is a BSD-licensed C++ library\\nwith Python and MATLAB bindings for training and deploying general-purpose\\nconvolutional neural networks and other deep models efficiently on commodity\\narchitectures. Caffe fits industry and internet-scale media needs by CUDA GPU\\ncomputation, processing over 40 million images a day on a single K40 or Titan\\nGPU ($\\\\approx$ 2.5 ms per image). By separating model representation from\\nactual implementation, Caffe allows experimentation and seamless switching\\namong platforms for ease of development and deployment from prototyping\\nmachines to cloud environments. Caffe is maintained and developed by the\\nBerkeley Vision and Learning Center (BVLC) with the help of an active community\\nof contributors on GitHub. It powers ongoing research projects, large-scale\\nindustrial applications, and startup prototypes in vision, speech, and\\nmultimedia.', 'title_of_article': 'Caffe: Convolutional Architecture for Fast Feature Embedding', 'type_of_entity': 'article', 'year_of_article': 2014, '_score': 87.18973}\n",
      "\n",
      "Biometrics\n",
      "{'_id': '1409.8212', '_index': 'arxiv_v0', 'booleanFlag_eu_article': False, 'booleanFlag_multinational_article': False, 'count_citations_article': 5, 'date_created_article': '2014-09-29', 'id_digitalObjectIdentifier_article': '10.1186/s13634-015-0255-5', 'json_fieldsOfStudy_article': {'nodes': [['Computer science'], ['Computer security', 'Machine learning', 'Theoretical computer science', 'Artificial intelligence'], ['Authentication', 'Encryption', 'Cryptography', 'Biometrics', 'Feature extraction'], ['Homomorphic encryption', 'Public-key cryptography', 'Authentication protocol']], 'links': [[[0, 0], [1, 0]], [[0, 0], [1, 1]], [[0, 0], [1, 2]], [[0, 0], [1, 3]], [[1, 0], [2, 0]], [[1, 0], [2, 1]], [[1, 0], [2, 2]], [[1, 0], [2, 3]], [[1, 1], [2, 4]], [[1, 2], [2, 2]], [[1, 3], [2, 3]], [[1, 3], [2, 4]], [[2, 0], [3, 2]], [[2, 1], [3, 0]], [[2, 1], [3, 1]]]}, 'terms_authors_article': ['Cagatay Karabat', 'Hakan Erdogan', 'Mehmet Sabir Kiraz', 'Erkay Savas'], 'terms_category_article': ['Cryptography and Security'], 'terms_countries_article': ['TR'], 'terms_fieldsOfStudy_article': ['Public-key cryptography', 'Machine learning', 'Computer security', 'Authentication', 'Theoretical computer science', 'Cryptography', 'Artificial intelligence', 'Homomorphic encryption', 'Authentication protocol', 'Encryption', 'Biometrics', 'Feature extraction', 'Computer science'], 'terms_institutes_article': ['Scientific And Technological Research Council Of Turkey', 'Sabancı University'], 'terms_regions_article': ['Western Asia'], 'terms_tokens_entity': ['extraction', 'plain', 'encryption', 'security', 'homomorphically', 'via', 'specification', 'feature', 'assumption', 'performed', 'using', 'private key', 'proof', 'pc', 'revealed', 'owner', 'deviate', 'enrollment', 'cryptosystem', 'model', 'ghz', 'template', 'call', 'prove', 'enhanced', 'used', 'therefore', 'stored', 'arbitrarily', 'never', 'needs', 'connection', 'homomorphic', 'biohash', 'protocols', 'involving', 'verification', 'data', 'original', 'time', 'verifier', 'randomized', 'since', 'ms', 'physical', 'based on', 'designed', 'protocol', 'templates', 'reveal', 'privacy', 'speed', 'efficiently', 'running', 'cannot', 'protection', 'encrypted', 'decryption', 'includes', 'binary', 'users', 'thus', 'link', 'paper', 'user', 'perform', 'real life', 'biometrics', 'overall', 'biometric', 'cheating', 'estimated', 'suitable', 'simulation based', 'authentication', 'proposed', 'mbit', 'scheme', 'two factor', 'shared', 'applications', 'on average', 'thrive', 'cpus', 'want to', 'output', 'may', 'threshold', \"user's\", 'modality', 'malicious', 'database', 'party', 'quad core', 'new', 'form', 'propose', 'binarized', 'system', 'desktop', 'consequently', 'novel', 'vectors', 'whose', 'preservation', 'stage', 'capability', 'presence'], 'textBody_abstract_article': \"In this paper, we propose a new biometric verification and template\\nprotection system which we call the THRIVE system. The system includes novel\\nenrollment and authentication protocols based on threshold homomorphic\\ncryptosystem where the private key is shared between a user and the verifier.\\nIn the THRIVE system, only encrypted binary biometric templates are stored in\\nthe database and verification is performed via homomorphically randomized\\ntemplates, thus, original templates are never revealed during the\\nauthentication stage. The THRIVE system is designed for the malicious model\\nwhere the cheating party may arbitrarily deviate from the protocol\\nspecification. Since threshold homomorphic encryption scheme is used, a\\nmalicious database owner cannot perform decryption on encrypted templates of\\nthe users in the database. Therefore, security of the THRIVE system is enhanced\\nusing a two-factor authentication scheme involving the user's private key and\\nthe biometric data. We prove security and privacy preservation capability of\\nthe proposed system in the simulation-based model with no assumption. The\\nproposed system is suitable for applications where the user does not want to\\nreveal her biometrics to the verifier in plain form but she needs to proof her\\nphysical presence by using biometrics. The system can be used with any\\nbiometric modality and biometric feature extraction scheme whose output\\ntemplates can be binarized. The overall connection time for the proposed THRIVE\\nsystem is estimated to be 336 ms on average for 256-bit biohash vectors on a\\ndesktop PC running with quad-core 3.2 GHz CPUs at 10 Mbit/s up/down link\\nconnection speed. Consequently, the proposed system can be efficiently used in\\nreal life applications.\", 'title_of_article': 'THRIVE: Threshold Homomorphic encryption based secure and privacy\\n  preserving bIometric VErification system', 'type_of_entity': 'article', 'year_of_article': 2014, '_score': 344.67596}\n",
      "\n",
      "Robotic Processes Automation\n"
     ]
    }
   ],
   "source": [
    "for term in [\"Natural Language Generation\",\n",
    "             \"Speech recognition\",\n",
    "             \"Virtual Agents\",\n",
    "             \"Machine Learning Platforms\",\n",
    "             \"AI-Optimized Hardware\",\n",
    "             \"Decision Management\",\n",
    "             \"Deep Learning Platforms\",\n",
    "             \"Biometrics\",\n",
    "             \"Robotic Processes Automation\",\n",
    "             \"Natural Language Processing\",\n",
    "             \"Digital Twin\",\n",
    "             \"Cyber Defense\",\n",
    "             \"Compliance\", \n",
    "             \"Knowledge Worker Aid\",\n",
    "             \"Content Creation\",\n",
    "             \"Peer to Peer Networks\",\n",
    "             \"Emotion Recognition\",\n",
    "             \"Image Recognition\",\n",
    "             \"Marketing Automation\"]:\n",
    "    print(term)\n",
    "    example_doc, data, all_scores = generate_indicator(term, indicator_family='ai_indicators')\n",
    "    print(example_doc)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
