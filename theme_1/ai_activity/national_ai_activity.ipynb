{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "#from query_indicators import generate_save_path\n",
    "from query_indicators import get_notebook_name\n",
    "from query_indicators import get_eu_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from collections import defaultdict\n",
    "from clio_lite import clio_search, clio_search_iter\n",
    "import io\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env variables\n",
    "mpl.rcParams['hatch.linewidth'] = 0.2\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['image.cmap'] = 'Pastel1'\n",
    "#os.environ['AWS_SHARED_CREDENTIALS_FILE'] = '/Users/jklinger/EURITO-AWS/.aws/credentials'  # <--- Note: NOT nesta's AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve current notebook path\n",
    "nb_name = get_notebook_name().replace('.ipynb', '')\n",
    "SAVE_PATH = nb_name.split(\"query_indicators/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some globals\n",
    "URL = \"https://search-eurito-prod-bbyn72q2rhx4ifj6h5dom43uhy.eu-west-1.es.amazonaws.com/\"\n",
    "INDEX = \"arxiv_v0\" \n",
    "FIELDS = ['terms_tokens_entity', 'textBody_abstract_article']\n",
    "EU_COUNTRIES = get_eu_countries() \n",
    "#fix: the above API produces erroneous list, so we clean it below\n",
    "EU_COUNTRIES = [e for e in EU_COUNTRIES if e not in (\"AX\", \"FO\", \"GF\", \"GI\", \"IM\")]\n",
    "COLORS = plt.get_cmap('Set2').colors\n",
    "COLOR_MAP = 'Pastel1'\n",
    "S3 = boto3.resource('s3')\n",
    "#SAVE_PATH = generate_save_path()  # EURITO collaborators: this is generated assuming you have stuck to the convention 'theme_x/something/something_else.ipynb'\n",
    "BUCKET = 'eurito-indicators'  # EURITO collaborators: please don't change this\n",
    "SAVE_RESULTS = False  # Set this to \"False\" when you want to view figures inline. When \"True\", results will be saved to S3.\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "    plt.ioff()  # <--- for turning off visible figs\n",
    "else:\n",
    "    plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theme_1/ai_activity/national_ai_activity'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_search(query, max_query_terms, yr0=2014, yr1=2019, countries=EU_COUNTRIES, window=1):\n",
    "    \"\"\"\n",
    "    Retrieve count and score data for a given basic clio search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Seed query for clio.\n",
    "        max_query_terms (list): Triple of max_query_terms (low, middle, high) to use from the initial query.\n",
    "        yr0 (int): Start year in range to use in filter.\n",
    "        yr1 (int): Final year in range to use in filter.\n",
    "        countries (list): A list of countries to filter (default to all EU).\n",
    "        window (int): The number of years to consider in between time windows. Note that changing this will lead to double-counting.\n",
    "    Returns:\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each country]}\n",
    "        all_scores (dict): {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "    \"\"\"\n",
    "    top_doc = None\n",
    "    _data = defaultdict(lambda: defaultdict(dict))  # {max_query_terms --> {year --> {country --> score} } }\n",
    "    all_scores = defaultdict(lambda: defaultdict(list))  # {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "    for n in max_query_terms:\n",
    "        # Set the order of the countries\n",
    "        for ctry in EU_COUNTRIES:\n",
    "            _data[n][ctry]\n",
    "            all_scores[n][ctry]\n",
    "        # Iterate over years\n",
    "        for yr in range(yr0, yr1+1):\n",
    "            # Set default values for countries\n",
    "            for ctry in EU_COUNTRIES:\n",
    "                _data[n][ctry][yr] = 0            \n",
    "            # Iterate over docs\n",
    "            filters = [{\"range\":{\"year_of_article\":{\"gte\":yr, \"lt\":yr+window}}}]\n",
    "            for doc in clio_search_iter(url=URL, index=INDEX, query=query, fields=FIELDS,\n",
    "                                        max_query_terms=n, post_filters=filters, chunksize=5000):\n",
    "                if '_score' not in doc or doc['terms_countries_article'] is None:\n",
    "                    continue\n",
    "                score = doc['_score']\n",
    "                for ctry in filter(lambda x: x in countries, doc['terms_countries_article']):\n",
    "                    if top_doc is None:\n",
    "                        top_doc = doc                \n",
    "                    all_scores[n][ctry].append(score)\n",
    "                    _data[n][ctry][yr] += score\n",
    "    # Reformat data as {max_query_terms --> [{year --> score} for each country in order]}\n",
    "    data = {}\n",
    "    for n, ctry_data in _data.items():\n",
    "        data[n] = []\n",
    "        for ctry, yr_data in ctry_data.items():\n",
    "            data[n].append(yr_data)\n",
    "    return top_doc, data, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator calculations\n",
    "\n",
    "Each of these functions is assumed to take the form\n",
    "\n",
    "```python\n",
    "def _an_indicator_calulation(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    A function calculating an indicator.\n",
    "    \n",
    "    Args:\n",
    "        data (list): Rows of data\n",
    "        year (int): A year to consider, if applicable.\n",
    "        _max (int): Divide by this to normalise your results. This is automatically applied in :obj:`make_activity_plot`\n",
    "    Returns:\n",
    "        result (list) A list of indicators to plot. The length of the list is assumed to be equal to the number of countries.\n",
    "    \"\"\"\n",
    "    # Calculate something\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _total_activity_by_country(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Sum of relevance scores, by year (if specified) or in total.\n",
    "    \"\"\"    \n",
    "    if year is None:        \n",
    "        scores = [sum(row.values())/_max for row in data]\n",
    "    else:\n",
    "        scores = [row[year]/_max for row in data]\n",
    "    return scores\n",
    "      \n",
    "\n",
    "def _average_activity_by_country(data, year=None, _max=1):    \n",
    "    \"\"\"\n",
    "    Indicator: Mean relevance score. This function is basically a lambda, since it assumes the average has already been calculated.\n",
    "    \"\"\"        \n",
    "    return [row/_max for row in data]\n",
    "    \n",
    "    \n",
    "def _corrected_average_activity_by_country(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Mean relevance score minus it's (very) approximate Poisson error.\n",
    "    \"\"\"    \n",
    "    return [(row - np.sqrt(row))/_max for row in data]\n",
    "    \n",
    "\n",
    "def _linear_coeffs(years, scores, _max):\n",
    "    \"\"\"Calculates linear coefficients for scores wrt years\"\"\"\n",
    "    return [np.polyfit(_scores, _years, 1)[0]/_max\n",
    "            if all(v > 0 for v in _scores) else 0\n",
    "            for _years, _scores in zip(years, scores)]    \n",
    "    \n",
    "\n",
    "def _trajectory(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Linear coefficient of total relevance score wrt year\n",
    "    \"\"\"\n",
    "    years = [list(row.keys()) for row in data]\n",
    "    print (\"years:\",years)\n",
    "    scores = [list(row.values()) for row in data]\n",
    "    print (\"scores:\",scores)\n",
    "    return _linear_coeffs(years, scores, _max)\n",
    "\n",
    "\n",
    "def _corrected_trajectory(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Linear coefficient of upper and lower limits of relevance score wrt year\n",
    "    \"\"\" \n",
    "    # Reformulate the data in terms of upper and lower bounds\n",
    "    years, scores = [], []\n",
    "    for row in data:\n",
    "        _years, _scores = [], []\n",
    "        for k, v in row.items():\n",
    "            _years += [k,k]\n",
    "            _scores += [v - np.sqrt(v), v + np.sqrt(v)]  # Estimate upper and lower limits with very approximate Poisson errors\n",
    "        years.append(_years)\n",
    "        scores.append(_scores)\n",
    "    return _linear_coeffs(years, scores, _max)\n",
    "\n",
    "def _concentration_technological_activity(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Share of total relevance score of a specified country in a sum of total relevance scores for all countries for a given technological query for a given time period.\n",
    "    \"\"\"\n",
    "    for row in data:\n",
    "        lta = _total_activity_by_country(row, year=None, _max=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Sorter:\n",
    "    def __init__(self, values, topn=None):\n",
    "        if topn is None:\n",
    "            topn = len(values)\n",
    "        self.indices = list(np.argsort(values))[-topn:]  # Argsort is ascending, so -ve indexing to pick up topn\n",
    "    def sort(self, x):\n",
    "        \"\"\"Sort list x by indices\"\"\"\n",
    "        return [x[i] for i in self.indices]\n",
    "\n",
    "\n",
    "def _s3_savefig(query, fig_name, extension='png'):\n",
    "    \"\"\"Save the figure to s3. The figure is grabbed from the global scope.\"\"\"\n",
    "    if not SAVE_RESULTS:\n",
    "        return    \n",
    "    outname = (f'figures/{SAVE_PATH}/'\n",
    "               f'{query.replace(\" \",\"_\").lower()}'\n",
    "               f'/{fig_name.replace(\" \",\"_\").lower()}'\n",
    "               f'.{extension}')\n",
    "    with io.BytesIO() as f:\n",
    "        plt.savefig(f, bbox_inches='tight', format=extension, pad_inches=0)\n",
    "        obj = S3.Object(BUCKET, outname)\n",
    "        f.seek(0)\n",
    "        obj.put(Body=f)\n",
    "\n",
    "        \n",
    "def _s3_savetable(data, key, index, object_path, transformer=lambda x: x):\n",
    "    \"\"\"Upload the table to s3\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(transformer(data[key]), index=index)\n",
    "    if len(df.columns) == 1:\n",
    "        df.columns = ['value']\n",
    "    print(object_path)\n",
    "    print(df)\n",
    "    if not SAVE_RESULTS:\n",
    "        return\n",
    "    df = df / df.max().max()\n",
    "    table_data = df.to_csv().encode()\n",
    "    obj = S3.Object(BUCKET, os.path.join(f'tables/{SAVE_PATH}', object_path))\n",
    "    obj.put(Body=table_data)\n",
    "\n",
    "        \n",
    "def make_activity_plot(f, data, countries, max_query_terms, query, \n",
    "                       year=None, label=None, x_padding=0.5, y_padding=0.05, xlabel_fontsize=14):\n",
    "    \"\"\"\n",
    "    Make a query and generate indicators by country, saving the plots to S3 and saving the rawest data\n",
    "    to tables on S3.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        f: An indicator function, as described in the 'Indicator calculations' section.\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each country]}\n",
    "        countries (list): A list of EU ISO-2 codes        \n",
    "        max_query_terms (list): Triple of max_query_terms for clio, corresponding to low, middle and high values of \n",
    "                                max_query_terms to test robustness of the query.\n",
    "        query (str): query used to generate this data.\n",
    "        year (int): Year to generate the indicator for (if applicable).\n",
    "        label (str): label for annotating the plot.\n",
    "        {x,y}_padding (float): Aesthetic padding around the extreme limits of the {x,y} axis.\n",
    "        xlabel_fontsize (int): Fontsize of the x labels (country ISO-2 codes).\n",
    "    \"\"\"    \n",
    "    # Calculate the indicator for each value of n, then recalculate the normalised indicator\n",
    "    _, middle, _ = (f(data[n], year=year) for n in max_query_terms)\n",
    "    low, middle, high = (f(data[n], year=year, _max=max(middle)) for n in max_query_terms)\n",
    "    indicator = [np.median([a, b, c]) for a, b, c in zip(low, middle, high)]    \n",
    "\n",
    "    # Sort all data by indicator value\n",
    "    s = _Sorter(indicator)\n",
    "    countries = s.sort(countries)\n",
    "    low = s.sort(low)\n",
    "    middle = s.sort(middle)\n",
    "    high =  s.sort(high)\n",
    "    indicator = s.sort(indicator)\n",
    "\n",
    "    # Make the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))    \n",
    "    make_error_boxes(ax, low, middle, high)  # Draw the bounding box\n",
    "    ax.scatter(countries, indicator,  s=0, marker='o', color='black')  # Draw the centre mark\n",
    "    ax.set_title(f'{label}\\nQuery: \"{query}\"')\n",
    "    ax.set_ylabel(label)\n",
    "\n",
    "    # Set limits and formulate \n",
    "    y0 = min(low+middle+high)    \n",
    "    y1 = max(low+middle+high)\n",
    "    if -y1*y_padding < y0:\n",
    "        y0 = -y1*y_padding\n",
    "    else:  # In case of negative values\n",
    "        y0 = y0 - np.abs(y0*y_padding)\n",
    "    ax.set_ylim(y0, y1*(1+y_padding))\n",
    "    ax.set_xlim(-x_padding, len(countries)-x_padding)\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(xlabel_fontsize)\n",
    "    \n",
    "    # Save to s3 & return\n",
    "    _s3_savefig(query, label)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def make_error_boxes(ax, low, middle, high, facecolor='r',\n",
    "                     edgecolor='None', alpha=0.5):\n",
    "    \"\"\"\n",
    "    Generate outer rectangles based on three values, and draw a horizontal line through the middle of the rectangle.\n",
    "    No assumption is made on the order of values, so don't worry if they're not properly ordered.\n",
    "        \n",
    "    Args:\n",
    "        ax (matplotlib.axis): An axis to add patches to.\n",
    "        {low, middle, high} (list): Three concurrent lists of values from which to calculate the rectangle limits.\n",
    "        {facecolor, edgecolor} (str): The {face,edge} colour of the rectangles.\n",
    "        alpha (float): The alpha of the rectangles.\n",
    "    \"\"\"\n",
    "    # Generate the rectangle\n",
    "    errorboxes = []\n",
    "    middlelines = []\n",
    "    for x, ys in enumerate(zip(low, middle, high)):        \n",
    "        rect = Rectangle((x - 0.45, min(ys)), 0.9, max(ys) - min(ys))\n",
    "        line = Rectangle((x - 0.45, np.median(ys)), 0.9, 0)\n",
    "        errorboxes.append(rect)\n",
    "        middlelines.append(line)\n",
    "\n",
    "    # Create patch collection with specified colour/alpha\n",
    "    pc = PatchCollection(errorboxes, facecolor=facecolor, alpha=alpha, edgecolor=edgecolor, hatch='/')\n",
    "    lc = PatchCollection(middlelines, facecolor='black', alpha=0.9, edgecolor='black')\n",
    "\n",
    "    # Add collection to axes\n",
    "    ax.add_collection(pc)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "\n",
    "def stacked_scores(all_scores, query, topn=8,\n",
    "                   low_bins=[10**i for i in np.arange(0, 1.1, 0.025)],\n",
    "                   high_bins=[10**i for i in np.arange(1.1, 2.5, 0.05)],\n",
    "                   x_scale='log', label='Relevance score breakdown', \n",
    "                   xlabel='Relevance score', ylabel='Number of relevant documents',\n",
    "                   legend_fontsize='small', legend_cols=2):\n",
    "    \"\"\"\n",
    "    Create stacked histogram of document scores by country. Two sets of bins are used, \n",
    "    in order to have a more legible binning scale.\n",
    "    \n",
    "    Args:\n",
    "        all_scores (dict): {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "        query (str): query used to generate this data.\n",
    "        low_bins (list): List of initial bin edges.\n",
    "        high_bins (list): List of supplementary bin edges. These could have a different spacing scheme to the lower bin edges.\n",
    "        x_scale (str): Argument for `ax.set_xscale`.\n",
    "        label (str): label for annotating the plot.\n",
    "        {x,y}_label (str): Argument for `ax.set_{x,y}label`.\n",
    "        legend_fontsize (str): Argument for legend fontsize.\n",
    "        legend_cols (str): Argument for legend ncol.        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort countries and scores by the sum of scores by country\n",
    "    countries = list(all_scores.keys())\n",
    "    scores = list(all_scores.values())    \n",
    "    s = _Sorter([sum(v) for v in scores], topn=topn)\n",
    "    scores = s.sort(scores)\n",
    "    countries = s.sort(countries)\n",
    "\n",
    "    # Plot the stacked scores\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.set_cmap(COLOR_MAP)\n",
    "    ax.hist(scores, bins=low_bins+high_bins, stacked=True,\n",
    "            label=countries, color=COLORS[:len(scores)])\n",
    "    \n",
    "    # Prettify the plot\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(fontsize=legend_fontsize, ncol=legend_cols)\n",
    "    ax.set_xlim(low_bins[0], None)\n",
    "    ax.set_xscale(x_scale)\n",
    "    ax.set_title(f'{label}\\nQuery: \"{query}\"')\n",
    "    \n",
    "    # Save to s3\n",
    "    _s3_savefig(query, label)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indicator(q, max_query_terms=[7, 10, 13], countries=EU_COUNTRIES, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Make a query and generate indicators by country, saving the plots to S3 and saving the rawest data\n",
    "    to tables on S3.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        q (str): The query to Elasticsearch\n",
    "        max_query_terms (list): Triple of max_query_terms for clio, corresponding to low, middle and high values of \n",
    "                                max_query_terms to test robustness of the query.\n",
    "        countries (list): A list of EU ISO-2 codes\n",
    "    Returns:\n",
    "        top_doc (dict): The highest ranking document from the search.\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each country]}\n",
    "        all_scores (dict): {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make the search and retrieve scores by country, and the highest ranking doc\n",
    "    example_doc, data, all_scores = make_search(q, max_query_terms=max_query_terms, countries=countries, *args, **kwargs)\n",
    "    \n",
    "    #print (\"data:\",data[max_query_terms[1]])\n",
    "    #print (\"all_scores:\",all_scores[max_query_terms[1]])\n",
    "        \n",
    "    total_sum = 0\n",
    "    country_scores = defaultdict(list)\n",
    "    for ctry in countries:\n",
    "        country_sum = 0\n",
    "        for n, _scores in all_scores.items():\n",
    "            sum_score = np.sum(_scores[ctry]) if len(_scores[ctry]) > 0 else 0 #sum all document scores for a country\n",
    "            country_sum = total_sum + sum_score\n",
    "        total_sum = total_sum + country_sum\n",
    "        country_scores[ctry].append(country_sum)\n",
    "    #print (\"country_scores:\",country_scores)\n",
    "\n",
    "    \n",
    "    #avg_score per year is not possible, due to all_scores not being divided by years, and data having only a sum of scores for each year\n",
    "    # Reformat the scores to calculate the average\n",
    "    avg_scores = defaultdict(list)\n",
    "    for ctry in countries:\n",
    "        for n, _scores in all_scores.items():\n",
    "            mean = np.mean(_scores[ctry]) if len(_scores[ctry]) > 0 else 0\n",
    "            avg_scores[n].append(mean)\n",
    "    #print (\"avg_scores:\", avg_scores)\n",
    "    \n",
    "    #first, let us get dict where dict[year] = sum of all country scores for that year - we will need to this to get CTA\n",
    "    year_sum = defaultdict(float)\n",
    "    for ctry_score in data[max_query_terms[1]]:\n",
    "        for year_score in ctry_score.items():\n",
    "            year_sum[year_score[0]] = year_sum[year_score[0]] + year_score[1]\n",
    "    #print (\"year_sum: \", year_sum)\n",
    "            \n",
    "    #calculate CTA by dividing LTA for one year by the sum of all country scores for that year\n",
    "    cta_scores_dict = defaultdict(list)\n",
    "    for mqt in max_query_terms:\n",
    "        for index, ctry_score in enumerate(data[mqt]): #for each country\n",
    "            ctry_cta_score = dict()\n",
    "            for year_score in ctry_score.items(): #for each year in a country\n",
    "                ctry_cta_score[year_score[0]] = year_score[1] / year_sum[year_score[0]]\n",
    "            cta_scores_dict[mqt].append(ctry_cta_score)\n",
    "    #print (\"cta_scores_dict: \", cta_scores_dict)\n",
    "    \n",
    "    plot_kwargs = dict(countries=countries, max_query_terms=max_query_terms, query=q)\n",
    "    # Calculate loads of indicators and save the plots\n",
    "    #_ = make_activity_plot(_total_activity_by_country, data, label='Total relevance score', **plot_kwargs)\n",
    "    #_ = make_activity_plot(_average_activity_by_country, avg_scores, label='Average relevance', **plot_kwargs)\n",
    "    #_ = make_activity_plot(_corrected_average_activity_by_country, avg_scores, label='Corrected average relevance',  **plot_kwargs)\n",
    "    #_ = make_activity_plot(_trajectory, data, label='Trajectory', **plot_kwargs)\n",
    "    #_ = make_activity_plot(_corrected_trajectory, data, label='Corrected trajectory', **plot_kwargs)\n",
    "    #_ = stacked_scores(all_scores[max_query_terms[1]], query=q)\n",
    "    \n",
    "    # Save the basic raw data as tables. Note: not as rich as the plotted data.\n",
    "    _q = q.replace(\" \",\"_\").lower()\n",
    "    _s3_savetable(data, max_query_terms[1], index=countries, object_path=f'{_q}/LTA.csv')\n",
    "    _s3_savetable(cta_scores_dict, max_query_terms[1], index=countries, object_path=f'{_q}/CTA.csv')\n",
    "    _s3_savetable(data, max_query_terms[1], transformer=_trajectory, index=countries, object_path=f'{_q}/TTA.csv')\n",
    "    _s3_savetable(avg_scores, max_query_terms[1], index=countries, object_path=f'{_q}/AA.csv')\n",
    "    \n",
    "    plt.close('all')  # Clean up the memory cache (unbelievable that matplotlib doesn't do this)\n",
    "    return example_doc, data, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Generation\n",
      "---------------------------\n",
      "natural_language_generation/LTA.csv\n",
      "            2014          2015          2016          2017          2018  \\\n",
      "AT   2426.861201   1781.139397   1662.517065   1780.543783   1517.401434   \n",
      "BE   2385.528763   3920.271069   5915.371682   7255.626801   8087.757232   \n",
      "BG     68.861133     67.093662     86.807829    302.707787      0.000000   \n",
      "HR    204.012774    366.626590    175.853212    374.351896    274.518432   \n",
      "CY     28.716194     11.009001     15.309305    158.578003     37.521760   \n",
      "CZ   1223.185917   1349.184406   2806.207409   3344.149874   3628.563505   \n",
      "DK   2157.451493   3635.479683   6348.033987   7252.655910   8554.309411   \n",
      "EE     30.845741     41.495632    252.573346    206.600721     33.858006   \n",
      "FI   1202.428015   2750.177992   4667.100376   4293.429199   5235.786208   \n",
      "FR   6488.393370   8382.403538  11027.914604  10960.421411  11194.579993   \n",
      "DE  10286.010687  11406.903188  13498.153608  15655.046147  14043.157705   \n",
      "GR    823.546931    543.913572    327.017945    768.409872    749.346760   \n",
      "HU    656.651712    389.667812    452.094648    480.187711    205.421779   \n",
      "IE   2207.989541   4658.458086   8052.457108  10061.789519  12690.142348   \n",
      "IT   4853.327585   5756.274794   7952.197373   7846.021832   7728.137203   \n",
      "LV    243.233996     98.099484    267.772260    170.761094      0.000000   \n",
      "LT      0.000000     23.935158     20.582294     54.115810      0.000000   \n",
      "LU    181.878255    183.486496     15.566925    193.295690     31.458332   \n",
      "MT    211.566134     89.037182    134.288880    445.483539    331.091949   \n",
      "NL   2391.229652   4914.422941   8123.459078   8746.238049  10295.101385   \n",
      "PL   1315.655608   1767.015159   2242.546670   2182.584803   1559.714980   \n",
      "PT   2201.399082   3317.376927   5487.686816   6302.439874   7696.856139   \n",
      "RO    114.741521    227.203545    276.608401    360.648356    591.665091   \n",
      "SK    204.671638    141.732997     69.304775     75.082093      0.000000   \n",
      "SI     69.222387    156.980688    197.126189    221.583253    229.036059   \n",
      "ES   3283.275182   4103.777867   4585.507748   4224.446139   3427.608574   \n",
      "SE   1536.350005   1943.671633   1471.148680   2172.490888   1699.038908   \n",
      "GB  11621.956547  15707.361273  21683.909183  22291.972989  24448.891129   \n",
      "\n",
      "            2019  \n",
      "AT    877.620121  \n",
      "BE   3250.636399  \n",
      "BG      0.000000  \n",
      "HR    140.807533  \n",
      "CY      0.000000  \n",
      "CZ   1756.510156  \n",
      "DK   3766.109463  \n",
      "EE     32.656986  \n",
      "FI   2107.919355  \n",
      "FR   4218.718189  \n",
      "DE   6283.458209  \n",
      "GR    499.687645  \n",
      "HU    236.999700  \n",
      "IE   6771.453120  \n",
      "IT   3972.622627  \n",
      "LV     96.672489  \n",
      "LT      0.000000  \n",
      "LU    129.320720  \n",
      "MT     74.953156  \n",
      "NL   4709.573317  \n",
      "PL    746.445215  \n",
      "PT   3272.414768  \n",
      "RO     74.299950  \n",
      "SK      0.000000  \n",
      "SI     68.046696  \n",
      "ES   1258.935396  \n",
      "SE    809.814110  \n",
      "GB  13882.465265  \n",
      "natural_language_generation/CTA.csv\n",
      "        2014      2015      2016      2017      2018      2019\n",
      "AT  0.041542  0.022913  0.015420  0.015066  0.012208  0.014865\n",
      "BE  0.040835  0.050432  0.054866  0.061394  0.065071  0.055060\n",
      "BG  0.001179  0.000863  0.000805  0.002561  0.000000  0.000000\n",
      "HR  0.003492  0.004716  0.001631  0.003168  0.002209  0.002385\n",
      "CY  0.000492  0.000142  0.000142  0.001342  0.000302  0.000000\n",
      "CZ  0.020938  0.017356  0.026028  0.028297  0.029194  0.029752\n",
      "DK  0.036931  0.046768  0.058879  0.061369  0.068825  0.063791\n",
      "EE  0.000528  0.000534  0.002343  0.001748  0.000272  0.000553\n",
      "FI  0.020583  0.035379  0.043288  0.036329  0.042125  0.035704\n",
      "FR  0.111067  0.107834  0.102285  0.092742  0.090068  0.071458\n",
      "DE  0.176073  0.146742  0.125197  0.132466  0.112986  0.106430\n",
      "GR  0.014097  0.006997  0.003033  0.006502  0.006029  0.008464\n",
      "HU  0.011240  0.005013  0.004193  0.004063  0.001653  0.004014\n",
      "IE  0.037796  0.059928  0.074688  0.085138  0.102100  0.114696\n",
      "IT  0.083078  0.074051  0.073758  0.066390  0.062178  0.067289\n",
      "LV  0.004164  0.001262  0.002484  0.001445  0.000000  0.001637\n",
      "LT  0.000000  0.000308  0.000191  0.000458  0.000000  0.000000\n",
      "LU  0.003113  0.002360  0.000144  0.001636  0.000253  0.002190\n",
      "MT  0.003622  0.001145  0.001246  0.003769  0.002664  0.001270\n",
      "NL  0.040932  0.063221  0.075346  0.074007  0.082831  0.079772\n",
      "PL  0.022521  0.022732  0.020800  0.018468  0.012549  0.012643\n",
      "PT  0.037683  0.042676  0.050899  0.053328  0.061926  0.055429\n",
      "RO  0.001964  0.002923  0.002566  0.003052  0.004760  0.001259\n",
      "SK  0.003504  0.001823  0.000643  0.000635  0.000000  0.000000\n",
      "SI  0.001185  0.002019  0.001828  0.001875  0.001843  0.001153\n",
      "ES  0.056202  0.052792  0.042531  0.035745  0.027577  0.021324\n",
      "SE  0.026299  0.025004  0.013645  0.018383  0.013670  0.013717\n",
      "GB  0.198941  0.202065  0.201121  0.188625  0.196707  0.235144\n",
      "years: [[2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019], [2014, 2015, 2016, 2017, 2018, 2019]]\n",
      "scores: [[2426.8612009999993, 1781.1393969999997, 1662.517065, 1780.5437829999998, 1517.401434, 877.6201209999999], [2385.5287630000007, 3920.271069000001, 5915.371682000001, 7255.626801, 8087.757231999998, 3250.6363989999986], [68.861133, 67.093662, 86.807829, 302.707787, 0, 0], [204.01277399999998, 366.62659, 175.853212, 374.351896, 274.518432, 140.807533], [28.716194, 11.009001, 15.309305, 158.578003, 37.52176, 0], [1223.1859174999997, 1349.1844059999996, 2806.207409, 3344.149874000002, 3628.5635049999996, 1756.510156], [2157.4514930000005, 3635.479682500003, 6348.033987000002, 7252.6559099999995, 8554.309411, 3766.109462999998], [30.845741, 41.495632, 252.57334600000002, 206.600721, 33.858006, 32.656986], [1202.428015, 2750.1779924999987, 4667.100375999999, 4293.429199, 5235.7862079999995, 2107.9193550000005], [6488.393369999995, 8382.403538000004, 11027.914604, 10960.421411000008, 11194.579993000001, 4218.718188999999], [10286.010686500009, 11406.9031875, 13498.153608000015, 15655.046147000017, 14043.157705000003, 6283.458209], [823.546931, 543.913572, 327.01794500000005, 768.409872, 749.3467599999999, 499.68764500000003], [656.6517124999999, 389.66781199999997, 452.09464799999995, 480.18771100000004, 205.421779, 236.9997], [2207.989540500001, 4658.458085999999, 8052.4571080000005, 10061.789518999998, 12690.142348000005, 6771.4531199999965], [4853.327584999998, 5756.2747935000025, 7952.197372999999, 7846.021832000003, 7728.137202999999, 3972.6226270000006], [243.233996, 98.099484, 267.77225999999996, 170.76109399999999, 0, 96.672489], [0, 23.935158, 20.582294, 54.115809999999996, 0, 0], [181.878255, 183.486496, 15.566925, 193.29568999999998, 31.458332, 129.32072], [211.566134, 89.037182, 134.28888, 445.48353900000006, 331.091949, 74.953156], [2391.229651500001, 4914.4229405000015, 8123.4590780000035, 8746.238049000001, 10295.101385, 4709.573316999998], [1315.6556079999993, 1767.0151594999998, 2242.54667, 2182.584803, 1559.7149799999993, 746.4452149999998], [2201.399082, 3317.376926500001, 5487.686816, 6302.439874, 7696.856138999996, 3272.4147679999987], [114.741521, 227.20354500000002, 276.608401, 360.64835600000004, 591.665091, 74.29995], [204.671638, 141.732997, 69.30477499999999, 75.082093, 0, 0], [69.222387, 156.98068800000001, 197.12618899999998, 221.58325299999998, 229.036059, 68.046696], [3283.275182000002, 4103.777867, 4585.507748000001, 4224.446139000002, 3427.608573999999, 1258.935396], [1536.3500054999997, 1943.671633, 1471.1486799999998, 2172.4908880000003, 1699.038908, 809.81411], [11621.956546500003, 15707.361272500022, 21683.909182999985, 22291.97298899998, 24448.891129000036, 13882.46526499999]]\n",
      "natural_language_generation/TTA.csv\n",
      "       value\n",
      "AT -0.003372\n",
      "BE  0.000344\n",
      "BG  0.000000\n",
      "HR -0.004020\n",
      "CY  0.000000\n",
      "CZ  0.000921\n",
      "DK  0.000389\n",
      "EE -0.000578\n",
      "FI  0.000457\n",
      "FR -0.000036\n",
      "DE -0.000089\n",
      "GR -0.001507\n",
      "HU -0.009403\n",
      "IE  0.000348\n",
      "IT  0.000047\n",
      "LV  0.000000\n",
      "LT  0.000000\n",
      "LU -0.008448\n",
      "MT  0.001631\n",
      "NL  0.000317\n",
      "PL -0.001114\n",
      "PT  0.000432\n",
      "RO  0.002773\n",
      "SK  0.000000\n",
      "SI  0.004415\n",
      "ES -0.000876\n",
      "SE -0.001665\n",
      "GB  0.000140\n",
      "natural_language_generation/AA.csv\n",
      "        value\n",
      "AT  32.198984\n",
      "BE  46.619050\n",
      "BG  27.656337\n",
      "HR  38.404261\n",
      "CY  27.903807\n",
      "CZ  45.953750\n",
      "DK  47.547286\n",
      "EE  35.178261\n",
      "FI  46.999631\n",
      "FR  39.902619\n",
      "DE  36.686974\n",
      "GR  34.369655\n",
      "HU  31.441862\n",
      "IE  49.711733\n",
      "IT  38.965830\n",
      "LV  54.783708\n",
      "LT  24.658316\n",
      "LU  38.684548\n",
      "MT  61.258135\n",
      "NL  45.985944\n",
      "PL  35.175493\n",
      "PT  48.587927\n",
      "RO  38.259695\n",
      "SK  24.539575\n",
      "SI  36.230587\n",
      "ES  34.461305\n",
      "SE  34.157852\n",
      "GB  42.793348\n",
      "A Deep Architecture for Semantic Parsing , 2014\n",
      "['GB', 'CA']\n",
      "Many successful approaches to semantic parsing build on top of the syntactic\n",
      "analysis of text, and make use of distributional representations or statistical\n",
      "models to match parses to ontology-specific queries. This paper presents a\n",
      "novel deep learning architecture which provides a semantic parsing system\n",
      "through the union of two neural models of language semantics. It allows for the\n",
      "generation of ontology-specific queries from natural language statements and\n",
      "questions without the need for parsing, which makes it especially suitable to\n",
      "grammatically malformed or syntactically atypical text, such as tweets, as well\n",
      "as permitting the development of semantic parsers for resource-poor languages.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "for term in [\"Natural Language Generation\",\n",
    "             \"Speech recognition\",\n",
    "             \"Virtual Agents\",\n",
    "             \"Machine Learning Platforms\",\n",
    "             \"AI-Optimized Hardware\",\n",
    "             \"Decision Management AI\",\n",
    "             \"Deep Learning Platforms\",\n",
    "             \"Biometrics AI\",\n",
    "             \"Robotic Processes Automation AI\",\n",
    "             \"Natural Language Processing\",\n",
    "             \"Digital Twin AI\",\n",
    "             \"Cyber Defense AI\",\n",
    "             \"Compliance AI\", \n",
    "             \"Knowledge Worker Aid AI\",\n",
    "             \"Content Creation AI\",\n",
    "             \"Peer to Peer Networks AI\",\n",
    "             \"Emotion Recognition AI\",\n",
    "             \"Image Recognition AI\",\n",
    "             \"Marketing Automation AI\"]:\n",
    "    \n",
    "    iteration += 1\n",
    "    if iteration > 1: #remove when not testing\n",
    "        break\n",
    "    \n",
    "    print(term)\n",
    "    print(\"-\"*len(term))\n",
    "    top_doc, data, all_scores = generate_indicator(term)\n",
    "    print(top_doc['title_of_article'], \",\", top_doc['year_of_article'])\n",
    "    print(top_doc['terms_countries_article'])\n",
    "    print(top_doc['textBody_abstract_article'])\n",
    "    print(\"\\n==============================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
