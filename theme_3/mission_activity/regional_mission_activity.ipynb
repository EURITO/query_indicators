{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "from query_indicators import generate_save_path\n",
    "from nuts_finder import NutsFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from collections import defaultdict\n",
    "from clio_lite import clio_search, clio_search_iter\n",
    "import io\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = NutsFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env variables\n",
    "mpl.rcParams['hatch.linewidth'] = 0.2\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['image.cmap'] = 'Pastel1'\n",
    "os.environ['AWS_SHARED_CREDENTIALS_FILE'] = '/Users/jklinger/EURITO-AWS/.aws/credentials'  # <--- Note: NOT nesta's AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some globals\n",
    "URL = \"https://search-eurito-prod-bbyn72q2rhx4ifj6h5dom43uhy.eu-west-1.es.amazonaws.com/\"\n",
    "INDEX = \"arxiv_v0\" \n",
    "FIELDS = ['terms_tokens_entity', 'textBody_abstract_article']\n",
    "COLORS = plt.get_cmap('Set2').colors\n",
    "COLOR_MAP = 'Pastel1'\n",
    "NUTS_REGIONS = {s['properties']['NUTS_ID']: s['properties']['NUTS_NAME'] \n",
    "               for s in sorted(nf.shapes['features'], key=lambda x: x['properties']['NUTS_ID'])\n",
    "               if s['properties']['LEVL_CODE'] == 1}\n",
    "S3 = boto3.resource('s3')\n",
    "SAVE_PATH = generate_save_path()  # EURITO collaborators: this is generated assuming you have stuck to the convention 'theme_x/something/something_else.ipynb'\n",
    "BUCKET = 'eurito-indicators'  # EURITO collaborators: please don't change this\n",
    "SAVE_RESULTS = True  # Set this to \"False\" when you want to view figures inline. When \"True\", results will be saved to S3.\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "    plt.ioff()  # <--- for turning off visible figs\n",
    "else:\n",
    "    plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_search(query, max_query_terms, yr0=2014, yr1=2019, regions=NUTS_REGIONS, window=1):\n",
    "    \"\"\"\n",
    "    Retrieve count and score data for a given basic clio search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Seed query for clio.\n",
    "        max_query_terms (list): Triple of max_query_terms (low, middle, high) to use from the initial query.\n",
    "        yr0 (int): Start year in range to use in filter.\n",
    "        yr1 (int): Final year in range to use in filter.\n",
    "        regions (list): A list of regions to filter (default to all EU).\n",
    "        window (int): The number of years to consider in between time windows. Note that changing this will lead to double-counting.\n",
    "    Returns:\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each region]}\n",
    "        all_scores (dict): {max_query_terms --> {region --> [score for doc in docs] } }\n",
    "    \"\"\"\n",
    "    top_doc = None\n",
    "    _data = defaultdict(lambda: defaultdict(dict))  # {max_query_terms --> {year --> {region --> score} } }\n",
    "    all_scores = defaultdict(lambda: defaultdict(list))  # {max_query_terms --> {region --> [score for doc in docs] } }\n",
    "    for n in max_query_terms:\n",
    "        # Set the order of the regions\n",
    "        for rgn in regions:\n",
    "            _data[n][rgn]\n",
    "            all_scores[n][rgn]\n",
    "        # Iterate over years\n",
    "        for yr in range(yr0, yr1+1):\n",
    "            # Set default values for regions\n",
    "            for rgn in regions:\n",
    "                _data[n][rgn][yr] = 0            \n",
    "            # Iterate over docs\n",
    "            filters = [{\"range\":{\"year_of_article\":{\"gte\":yr, \"lt\":yr+window}}}]\n",
    "            for doc in clio_search_iter(url=URL, index=INDEX, query=query, fields=FIELDS,\n",
    "                                        max_query_terms=n, post_filters=filters, chunksize=5000):\n",
    "                if ('_score' not in doc) or ('terms_nuts1_article' not in doc) or (doc['terms_nuts1_article'] is None):\n",
    "                    continue\n",
    "                score = doc['_score']                \n",
    "                for rgn in filter(lambda x: x in regions, doc['terms_nuts1_article']):\n",
    "                    if top_doc is None:\n",
    "                        top_doc = doc\n",
    "                    all_scores[n][rgn].append(score)\n",
    "                    _data[n][rgn][yr] += score\n",
    "    # Reformat data as {max_query_terms --> [{year --> score} for each region in order]}\n",
    "    data = {}\n",
    "    for n, regn_data in _data.items():\n",
    "        data[n] = []\n",
    "        for rgn, yr_data in regn_data.items():\n",
    "            data[n].append(yr_data)\n",
    "    return top_doc, data, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator calculations\n",
    "\n",
    "Each of these functions is assumed to take the form\n",
    "\n",
    "```python\n",
    "def _an_indicator_calulation(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    A function calculating an indicator.\n",
    "    \n",
    "    Args:\n",
    "        data (list): Rows of data\n",
    "        year (int): A year to consider, if applicable.\n",
    "        _max (int): Divide by this to normalise your results. This is automatically applied in :obj:`make_activity_plot`\n",
    "    Returns:\n",
    "        result (list) A list of indicators to plot. The length of the list is assumed to be equal to the number of regions.\n",
    "    \"\"\"\n",
    "    # Calculate something\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _total_activity_by_region(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Sum of relevance scores, by year (if specified) or in total.\n",
    "    \"\"\"    \n",
    "    if year is None:        \n",
    "        scores = [sum(row.values())/_max for row in data]\n",
    "    else:\n",
    "        scores = [row[year]/_max for row in data]\n",
    "    return scores\n",
    "      \n",
    "\n",
    "def _average_activity_by_region(data, year=None, _max=1):    \n",
    "    \"\"\"\n",
    "    Indicator: Mean relevance score. This function is basically a lambda, since it assumes the average has already been calculated.\n",
    "    \"\"\"        \n",
    "    return [row/_max for row in data]\n",
    "    \n",
    "    \n",
    "def _corrected_average_activity_by_region(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Mean relevance score minus it's (very) approximate Poisson error.\n",
    "    \"\"\"    \n",
    "    return [(row - np.sqrt(row))/_max for row in data]\n",
    "    \n",
    "\n",
    "def _linear_coeffs(years, scores, _max):\n",
    "    \"\"\"Calculates linear coefficients for scores wrt years\"\"\"\n",
    "    return [np.polyfit(_scores, _years, 1)[0]/_max\n",
    "            if all(v > 0 for v in _scores) else 0\n",
    "            for _years, _scores in zip(years, scores)]    \n",
    "    \n",
    "\n",
    "def _trajectory(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Linear coefficient of total relevance score wrt year\n",
    "    \"\"\"\n",
    "    years = [list(row.keys()) for row in data]\n",
    "    scores = [list(row.values()) for row in data]\n",
    "    return _linear_coeffs(years, scores, _max)\n",
    "\n",
    "\n",
    "def _corrected_trajectory(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Linear coefficient of upper and lower limits of relevance score wrt year\n",
    "    \"\"\" \n",
    "    # Reformulate the data in terms of upper and lower bounds\n",
    "    years, scores = [], []\n",
    "    for row in data:\n",
    "        _years, _scores = [], []\n",
    "        for k, v in row.items():\n",
    "            _years += [k,k]\n",
    "            _scores += [v - np.sqrt(v), v + np.sqrt(v)]  # Estimate upper and lower limits with very approximate Poisson errors\n",
    "        years.append(_years)\n",
    "        scores.append(_scores)\n",
    "    return _linear_coeffs(years, scores, _max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_chunks(*args):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    n = 30\n",
    "    l = args[0]\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (x[i:i+n] for x in args)\n",
    "\n",
    "        \n",
    "class _Sorter:\n",
    "    def __init__(self, values, topn=None):\n",
    "        values = list(values)\n",
    "        if topn is None:\n",
    "            topn = len(values)\n",
    "        self.indices = list(np.argsort(values))[-topn:]  # Argsort is ascending, so -ve indexing to pick up topn\n",
    "    def sort(self, x):\n",
    "        \"\"\"Sort list x by indices\"\"\"\n",
    "        x = list(x)\n",
    "        return [x[i] for i in self.indices]\n",
    "\n",
    "\n",
    "def _s3_savefig(query, fig_name, extension='png'):\n",
    "    \"\"\"Save the figure to s3. The figure is grabbed from the global scope.\"\"\"\n",
    "    if not SAVE_RESULTS:\n",
    "        return    \n",
    "    outname = (f'figures/{SAVE_PATH}/'\n",
    "               f'{query.replace(\" \",\"_\").lower()}'\n",
    "               f'/{fig_name.replace(\" \",\"_\").lower()}'\n",
    "               f'.{extension}')\n",
    "    with io.BytesIO() as f:\n",
    "        plt.savefig(f, bbox_inches='tight', format=extension, pad_inches=0)\n",
    "        obj = S3.Object(BUCKET, outname)\n",
    "        f.seek(0)\n",
    "        obj.put(Body=f)\n",
    "\n",
    "        \n",
    "def _s3_savetable(data, key, index, object_path, transformer=lambda x: x):\n",
    "    \"\"\"Upload the table to s3\"\"\"\n",
    "    if not SAVE_RESULTS:\n",
    "        return\n",
    "    df = pd.DataFrame(transformer(data[key]), index=index)\n",
    "    if len(df.columns) == 1:\n",
    "        df.columns = ['value']\n",
    "    df = df / df.max().max()\n",
    "    table_data = df.to_csv().encode()\n",
    "    obj = S3.Object(BUCKET, os.path.join(f'tables/{SAVE_PATH}', object_path))\n",
    "    obj.put(Body=table_data)\n",
    "\n",
    "        \n",
    "def make_activity_plot(f, data, regions, max_query_terms, query, \n",
    "                       year=None, label=None, x_padding=0.5, y_padding=0.05, xlabel_fontsize=14):\n",
    "    \"\"\"\n",
    "    Make a query and generate indicators by region, saving the plots to S3 and saving the rawest data\n",
    "    to tables on S3.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        f: An indicator function, as described in the 'Indicator calculations' section.\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each region]}\n",
    "        regions (list): A list of NUTS-1 codes        \n",
    "        max_query_terms (list): Triple of max_query_terms for clio, corresponding to low, middle and high values of \n",
    "                                max_query_terms to test robustness of the query.\n",
    "        query (str): query used to generate this data.\n",
    "        year (int): Year to generate the indicator for (if applicable).\n",
    "        label (str): label for annotating the plot.\n",
    "        {x,y}_padding (float): Aesthetic padding around the extreme limits of the {x,y} axis.\n",
    "        xlabel_fontsize (int): Fontsize of the x labels (region NUTS1 codes).\n",
    "    \"\"\"    \n",
    "    # Calculate the indicator for each value of n, then recalculate the normalised indicator\n",
    "    _, middle, _ = (f(data[n], year=year) for n in max_query_terms)\n",
    "    low, middle, high = (f(data[n], year=year, _max=max(middle)) for n in max_query_terms)\n",
    "    indicator = [np.median([a, b, c]) for a, b, c in zip(low, middle, high)]    \n",
    "\n",
    "    # Sort all data by indicator value\n",
    "    s = _Sorter(indicator)\n",
    "    regions = [NUTS_REGIONS[r] for r in s.sort(regions)]\n",
    "    low = s.sort(low)\n",
    "    middle = s.sort(middle)\n",
    "    high =  s.sort(high)\n",
    "    indicator = s.sort(indicator)\n",
    "\n",
    "    i = 0\n",
    "    for _regions, _low, _middle, _high, _indicator in zip_chunks(regions, low, middle, high, indicator):\n",
    "        j = len(_regions)\n",
    "        # Make the scatter plot\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))    \n",
    "        make_error_boxes(ax, _low, _middle, _high)  # Draw the bounding box\n",
    "        ax.scatter(_regions, _indicator,  s=0, marker='o', color='black')  # Draw the centre mark\n",
    "        ax.set_title(f'{label}\\nQuery: \"{query}\"\\nResults {i} to {i+j}')\n",
    "        ax.set_ylabel(label)\n",
    "\n",
    "        # Set limits and formulate \n",
    "        y0 = min(_low+_middle+_high)    \n",
    "        y1 = max(_low+_middle+_high)\n",
    "        if -y1*y_padding < y0:\n",
    "            y0 = -y1*y_padding\n",
    "        else:  # In case of negative values\n",
    "            y0 = y0 - np.abs(y0*y_padding)\n",
    "        ax.set_ylim(y0, y1*(1+y_padding))\n",
    "        ax.set_xlim(-x_padding, len(_regions)-x_padding)\n",
    "\n",
    "        ax.set_xticklabels(_regions, rotation=40, ha='right', fontsize=xlabel_fontsize)\n",
    "        #for tick in ax.xaxis.get_major_ticks():\n",
    "        #    tick.label.set_fontsize(xlabel_fontsize)\n",
    "        #    tick.label.set_rotation(45, ha='right')\n",
    "            \n",
    "        # Save to s3 & return\n",
    "        _s3_savefig(f'{query}_{i}_{j}', label)\n",
    "        i += j\n",
    "        \n",
    "    return ax\n",
    "\n",
    "\n",
    "def make_error_boxes(ax, low, middle, high, facecolor='r',\n",
    "                     edgecolor='None', alpha=0.5):\n",
    "    \"\"\"\n",
    "    Generate outer rectangles based on three values, and draw a horizontal line through the middle of the rectangle.\n",
    "    No assumption is made on the order of values, so don't worry if they're not properly ordered.\n",
    "        \n",
    "    Args:\n",
    "        ax (matplotlib.axis): An axis to add patches to.\n",
    "        {low, middle, high} (list): Three concurrent lists of values from which to calculate the rectangle limits.\n",
    "        {facecolor, edgecolor} (str): The {face,edge} colour of the rectangles.\n",
    "        alpha (float): The alpha of the rectangles.\n",
    "    \"\"\"\n",
    "    # Generate the rectangle\n",
    "    errorboxes = []\n",
    "    middlelines = []\n",
    "    for x, ys in enumerate(zip(low, middle, high)):        \n",
    "        rect = Rectangle((x - 0.45, min(ys)), 0.9, max(ys) - min(ys))\n",
    "        line = Rectangle((x - 0.45, np.median(ys)), 0.9, 0)\n",
    "        errorboxes.append(rect)\n",
    "        middlelines.append(line)\n",
    "\n",
    "    # Create patch collection with specified colour/alpha\n",
    "    pc = PatchCollection(errorboxes, facecolor=facecolor, alpha=alpha, edgecolor=edgecolor, hatch='/')\n",
    "    lc = PatchCollection(middlelines, facecolor='black', alpha=0.9, edgecolor='black')\n",
    "\n",
    "    # Add collection to axes\n",
    "    ax.add_collection(pc)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "\n",
    "def stacked_scores(all_scores, query, topn=8,\n",
    "                   low_bins=[10**i for i in np.arange(0, 1.1, 0.025)],\n",
    "                   high_bins=[10**i for i in np.arange(1.1, 2.5, 0.05)],\n",
    "                   x_scale='log', label='Relevance score breakdown', \n",
    "                   xlabel='Relevance score', ylabel='Number of relevant documents',\n",
    "                   legend_fontsize='small', legend_cols=2):\n",
    "    \"\"\"\n",
    "    Create stacked histogram of document scores by region. Two sets of bins are used, \n",
    "    in order to have a more legible binning scale.\n",
    "    \n",
    "    Args:\n",
    "        all_scores (dict): {max_query_terms --> {region --> [score for doc in docs] } }\n",
    "        query (str): query used to generate this data.\n",
    "        low_bins (list): List of initial bin edges.\n",
    "        high_bins (list): List of supplementary bin edges. These could have a different spacing scheme to the lower bin edges.\n",
    "        x_scale (str): Argument for `ax.set_xscale`.\n",
    "        label (str): label for annotating the plot.\n",
    "        {x,y}_label (str): Argument for `ax.set_{x,y}label`.\n",
    "        legend_fontsize (str): Argument for legend fontsize.\n",
    "        legend_cols (str): Argument for legend ncol.        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort regions and scores by the sum of scores by reguion\n",
    "    regions = list(all_scores.keys())\n",
    "    scores = list(all_scores.values())    \n",
    "    s = _Sorter([sum(v) for v in scores], topn=topn)\n",
    "    scores = s.sort(scores)\n",
    "    regions = [NUTS_REGIONS[r] for r in s.sort(regions)]\n",
    "\n",
    "    # Plot the stacked scores\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.set_cmap(COLOR_MAP)\n",
    "    ax.hist(scores, bins=low_bins+high_bins, stacked=True,\n",
    "            label=regions, color=COLORS[:len(scores)])\n",
    "    \n",
    "    # Prettify the plot\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(fontsize=legend_fontsize, ncol=legend_cols)\n",
    "    ax.set_xlim(low_bins[0], None)    \n",
    "    y0, y1 = ax.get_ylim()\n",
    "    ax.set_ylim(y0, y1*1.1)\n",
    "    ax.set_xscale(x_scale)\n",
    "    ax.set_title(f'{label}\\nQuery: \"{query}\"')\n",
    "    \n",
    "    # Save to s3\n",
    "    _s3_savefig(query, label)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indicator(q, max_query_terms=[7, 10, 13], regions=NUTS_REGIONS, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Make a query and generate indicators by region, saving the plots to S3 and saving the rawest data\n",
    "    to tables on S3.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        q (str): The query to Elasticsearch\n",
    "        max_query_terms (list): Triple of max_query_terms for clio, corresponding to low, middle and high values of \n",
    "                                max_query_terms to test robustness of the query.\n",
    "        regions (list): A list of EU ISO-2 codes\n",
    "    Returns:\n",
    "        top_doc (dict): The highest ranking document from the search.\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each region]}\n",
    "        all_scores (dict): {max_query_terms --> {region --> [score for doc in docs] } }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make the search and retrieve scores by region, and the highest ranking doc\n",
    "    example_doc, data, all_scores = make_search(q, max_query_terms=max_query_terms, regions=regions, *args, **kwargs)\n",
    "\n",
    "    # Reformat the scores to calculate the average\n",
    "    avg_scores = defaultdict(list)\n",
    "    for rgn in regions:\n",
    "        for n, _scores in all_scores.items():\n",
    "            mean = np.mean(_scores[rgn]) if len(_scores[rgn]) > 0 else 0\n",
    "            avg_scores[n].append(mean)\n",
    "    \n",
    "    plot_kwargs = dict(regions=regions, max_query_terms=max_query_terms, query=q)\n",
    "    # Calculate loads of indicators and save the plots\n",
    "    _ = make_activity_plot(_total_activity_by_region, data, label='Total relevance score', **plot_kwargs)\n",
    "    _ = make_activity_plot(_average_activity_by_region, avg_scores, label='Average relevance', **plot_kwargs)\n",
    "    _ = make_activity_plot(_corrected_average_activity_by_region, avg_scores, label='Corrected average relevance',  **plot_kwargs)\n",
    "    _ = make_activity_plot(_trajectory, data, label='Trajectory', **plot_kwargs)\n",
    "    _ = make_activity_plot(_corrected_trajectory, data, label='Corrected trajectory', **plot_kwargs)\n",
    "    _ = stacked_scores(all_scores[max_query_terms[1]], query=q)\n",
    "    \n",
    "    # Save the basic raw data as tables. Note: not as rich as the plotted data.\n",
    "    _q = q.replace(\" \",\"_\").lower()\n",
    "    _s3_savetable(data, max_query_terms[1], index=regions, object_path=f'{_q}/total_relevance.csv')\n",
    "    _s3_savetable(avg_scores, max_query_terms[1], index=regions, object_path=f'{_q}/avg_relevance.csv')\n",
    "    _s3_savetable(data, max_query_terms[1], transformer=_trajectory, index=regions, object_path=f'{_q}/trajectory.csv')\n",
    "    \n",
    "    if SAVE_RESULTS:\n",
    "        plt.close('all')  # Clean up the memory cache (unbelievable that matplotlib doesn't do this)\n",
    "    return example_doc, data, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Generation\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jklinger/anaconda3/envs/cliotest/lib/python3.6/site-packages/ipykernel_launcher.py:85: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "/Users/jklinger/anaconda3/envs/cliotest/lib/python3.6/site-packages/ipykernel_launcher.py:173: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Deep Architecture for Semantic Parsing , 2014\n",
      "['UKJ']\n",
      "Many successful approaches to semantic parsing build on top of the syntactic\n",
      "analysis of text, and make use of distributional representations or statistical\n",
      "models to match parses to ontology-specific queries. This paper presents a\n",
      "novel deep learning architecture which provides a semantic parsing system\n",
      "through the union of two neural models of language semantics. It allows for the\n",
      "generation of ontology-specific queries from natural language statements and\n",
      "questions without the need for parsing, which makes it especially suitable to\n",
      "grammatically malformed or syntactically atypical text, such as tweets, as well\n",
      "as permitting the development of semantic parsers for resource-poor languages.\n",
      "\n",
      "==============================\n",
      "\n",
      "Speech recognition\n",
      "------------------\n",
      "Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy\n",
      "  and Reverberant Environments , 2014\n",
      "['DE2']\n",
      "We propose a spatial diffuseness feature for deep neural network (DNN)-based\n",
      "automatic speech recognition to improve recognition accuracy in reverberant and\n",
      "noisy environments. The feature is computed in real-time from multiple\n",
      "microphone signals without requiring knowledge or estimation of the direction\n",
      "of arrival, and represents the relative amount of diffuse noise in each time\n",
      "and frequency bin. It is shown that using the diffuseness feature as an\n",
      "additional input to a DNN-based acoustic model leads to a reduced word error\n",
      "rate for the REVERB challenge corpus, both compared to logmelspec features\n",
      "extracted from noisy signals, and features enhanced by spectral subtraction.\n",
      "\n",
      "==============================\n",
      "\n",
      "Virtual Agents\n",
      "--------------\n",
      "Expressing social attitudes in virtual agents for social training games , 2014\n",
      "['FR1']\n",
      "The use of virtual agents in social coaching has increased rapidly in the\n",
      "last decade. In order to train the user in different situations than can occur\n",
      "in real life, the virtual agent should be able to express different social\n",
      "attitudes. In this paper, we propose a model of social attitudes that enables a\n",
      "virtual agent to reason on the appropriate social attitude to express during\n",
      "the interaction with a user given the course of the interaction, but also the\n",
      "emotions, mood and personality of the agent. Moreover, the model enables the\n",
      "virtual agent to display its social attitude through its non-verbal behaviour.\n",
      "The proposed model has been developed in the context of job interview\n",
      "simulation. The methodology used to develop such a model combined a theoretical\n",
      "and an empirical approach. Indeed, the model is based both on the literature in\n",
      "Human and Social Sciences on social attitudes but also on the analysis of an\n",
      "audiovisual corpus of job interviews and on post-hoc interviews with the\n",
      "recruiters on their expressed attitudes during the job interview.\n",
      "\n",
      "==============================\n",
      "\n",
      "Machine Learning Platforms\n",
      "--------------------------\n",
      "Open science in machine learning , 2014\n",
      "['DE3', 'DE4', 'NL3']\n",
      "We present OpenML and mldata, open science platforms that provides easy\n",
      "access to machine learning data, software and results to encourage further\n",
      "study and application. They go beyond the more traditional repositories for\n",
      "data sets and software packages in that they allow researchers to also easily\n",
      "share the results they obtained in experiments and to compare their solutions\n",
      "with those of others.\n",
      "\n",
      "==============================\n",
      "\n",
      "AI-Optimized Hardware\n",
      "---------------------\n",
      "Hands-on experiments on intelligent behavior for mobile robots , 2014\n",
      "['DE4', 'DE3']\n",
      "In recent years, Artificial Intelligence techniques have emerged as useful\n",
      "tools for solving various engineering problems that were not possible or\n",
      "convenient to handle by traditional methods. AI has directly influenced many\n",
      "areas of computer science and becomes an important part of the engineering\n",
      "curriculum. However, determining the important topics for a single semester AI\n",
      "course is a nontrivial task, given the lack of a general methodology. AI\n",
      "concepts commonly overlap with many other disciplines involving a wide range of\n",
      "subjects, including applied approaches to more formal mathematical issues. This\n",
      "paper presents the use of a simple robotic platform to assist the learning of\n",
      "basic AI concepts. The study is guided through some simple experiments using\n",
      "autonomous mobile robots. The central algorithm is the Learning Automata. Using\n",
      "LA, each robot action is applied to an environment to be evaluated by means of\n",
      "a fitness value. The response of the environment is used by the automata to\n",
      "select its next action. This procedure holds until the goal task is reached.\n",
      "The proposal addresses the AI study by offering in LA a unifying context to\n",
      "draw together several of the topics of AI and motivating the students to learn\n",
      "by building some hands on laboratory exercises. The presented material has been\n",
      "successfully tested as AI teaching aide in the University of Guadalajara\n",
      "robotics group as it motivates students and increases enrolment and retention\n",
      "while educating better computer engineers.\n",
      "\n",
      "==============================\n",
      "\n",
      "Decision Management AI\n",
      "----------------------\n",
      "Hands-on experiments on intelligent behavior for mobile robots , 2014\n",
      "['DE4', 'DE3']\n",
      "In recent years, Artificial Intelligence techniques have emerged as useful\n",
      "tools for solving various engineering problems that were not possible or\n",
      "convenient to handle by traditional methods. AI has directly influenced many\n",
      "areas of computer science and becomes an important part of the engineering\n",
      "curriculum. However, determining the important topics for a single semester AI\n",
      "course is a nontrivial task, given the lack of a general methodology. AI\n",
      "concepts commonly overlap with many other disciplines involving a wide range of\n",
      "subjects, including applied approaches to more formal mathematical issues. This\n",
      "paper presents the use of a simple robotic platform to assist the learning of\n",
      "basic AI concepts. The study is guided through some simple experiments using\n",
      "autonomous mobile robots. The central algorithm is the Learning Automata. Using\n",
      "LA, each robot action is applied to an environment to be evaluated by means of\n",
      "a fitness value. The response of the environment is used by the automata to\n",
      "select its next action. This procedure holds until the goal task is reached.\n",
      "The proposal addresses the AI study by offering in LA a unifying context to\n",
      "draw together several of the topics of AI and motivating the students to learn\n",
      "by building some hands on laboratory exercises. The presented material has been\n",
      "successfully tested as AI teaching aide in the University of Guadalajara\n",
      "robotics group as it motivates students and increases enrolment and retention\n",
      "while educating better computer engineers.\n",
      "\n",
      "==============================\n",
      "\n",
      "Deep Learning Platforms\n",
      "-----------------------\n",
      "Caffe: Convolutional Architecture for Fast Feature Embedding , 2014\n",
      "['UKI', 'IE0', 'CH0', 'UKJ', 'UKH']\n",
      "Caffe provides multimedia scientists and practitioners with a clean and\n",
      "modifiable framework for state-of-the-art deep learning algorithms and a\n",
      "collection of reference models. The framework is a BSD-licensed C++ library\n",
      "with Python and MATLAB bindings for training and deploying general-purpose\n",
      "convolutional neural networks and other deep models efficiently on commodity\n",
      "architectures. Caffe fits industry and internet-scale media needs by CUDA GPU\n",
      "computation, processing over 40 million images a day on a single K40 or Titan\n",
      "GPU ($\\approx$ 2.5 ms per image). By separating model representation from\n",
      "actual implementation, Caffe allows experimentation and seamless switching\n",
      "among platforms for ease of development and deployment from prototyping\n",
      "machines to cloud environments. Caffe is maintained and developed by the\n",
      "Berkeley Vision and Learning Center (BVLC) with the help of an active community\n",
      "of contributors on GitHub. It powers ongoing research projects, large-scale\n",
      "industrial applications, and startup prototypes in vision, speech, and\n",
      "multimedia.\n",
      "\n",
      "==============================\n",
      "\n",
      "Biometrics AI\n",
      "-------------\n",
      "THRIVE: Threshold Homomorphic encryption based secure and privacy\n",
      "  preserving bIometric VErification system , 2014\n",
      "['TR1', 'TR5', 'TR4']\n",
      "In this paper, we propose a new biometric verification and template\n",
      "protection system which we call the THRIVE system. The system includes novel\n",
      "enrollment and authentication protocols based on threshold homomorphic\n",
      "cryptosystem where the private key is shared between a user and the verifier.\n",
      "In the THRIVE system, only encrypted binary biometric templates are stored in\n",
      "the database and verification is performed via homomorphically randomized\n",
      "templates, thus, original templates are never revealed during the\n",
      "authentication stage. The THRIVE system is designed for the malicious model\n",
      "where the cheating party may arbitrarily deviate from the protocol\n",
      "specification. Since threshold homomorphic encryption scheme is used, a\n",
      "malicious database owner cannot perform decryption on encrypted templates of\n",
      "the users in the database. Therefore, security of the THRIVE system is enhanced\n",
      "using a two-factor authentication scheme involving the user's private key and\n",
      "the biometric data. We prove security and privacy preservation capability of\n",
      "the proposed system in the simulation-based model with no assumption. The\n",
      "proposed system is suitable for applications where the user does not want to\n",
      "reveal her biometrics to the verifier in plain form but she needs to proof her\n",
      "physical presence by using biometrics. The system can be used with any\n",
      "biometric modality and biometric feature extraction scheme whose output\n",
      "templates can be binarized. The overall connection time for the proposed THRIVE\n",
      "system is estimated to be 336 ms on average for 256-bit biohash vectors on a\n",
      "desktop PC running with quad-core 3.2 GHz CPUs at 10 Mbit/s up/down link\n",
      "connection speed. Consequently, the proposed system can be efficiently used in\n",
      "real life applications.\n",
      "\n",
      "==============================\n",
      "\n",
      "Robotic Processes Automation AI\n",
      "-------------------------------\n",
      "Hands-on experiments on intelligent behavior for mobile robots , 2014\n",
      "['DE4', 'DE3']\n",
      "In recent years, Artificial Intelligence techniques have emerged as useful\n",
      "tools for solving various engineering problems that were not possible or\n",
      "convenient to handle by traditional methods. AI has directly influenced many\n",
      "areas of computer science and becomes an important part of the engineering\n",
      "curriculum. However, determining the important topics for a single semester AI\n",
      "course is a nontrivial task, given the lack of a general methodology. AI\n",
      "concepts commonly overlap with many other disciplines involving a wide range of\n",
      "subjects, including applied approaches to more formal mathematical issues. This\n",
      "paper presents the use of a simple robotic platform to assist the learning of\n",
      "basic AI concepts. The study is guided through some simple experiments using\n",
      "autonomous mobile robots. The central algorithm is the Learning Automata. Using\n",
      "LA, each robot action is applied to an environment to be evaluated by means of\n",
      "a fitness value. The response of the environment is used by the automata to\n",
      "select its next action. This procedure holds until the goal task is reached.\n",
      "The proposal addresses the AI study by offering in LA a unifying context to\n",
      "draw together several of the topics of AI and motivating the students to learn\n",
      "by building some hands on laboratory exercises. The presented material has been\n",
      "successfully tested as AI teaching aide in the University of Guadalajara\n",
      "robotics group as it motivates students and increases enrolment and retention\n",
      "while educating better computer engineers.\n",
      "\n",
      "==============================\n",
      "\n",
      "Natural Language Processing\n",
      "---------------------------\n",
      "Linguistic Analysis of Requirements of a Space Project and their\n",
      "  Conformity with the Recommendations Proposed by a Controlled Natural Language , 2014\n",
      "['FRJ']\n",
      "The long term aim of the project carried out by the French National Space\n",
      "Agency (CNES) is to design a writing guide based on the real and regular\n",
      "writing of requirements. As a first step in the project, this paper proposes a\n",
      "lin-guistic analysis of requirements written in French by CNES engineers. The\n",
      "aim is to determine to what extent they conform to two rules laid down in\n",
      "INCOSE, a recent guide for writing requirements. Although CNES engineers are\n",
      "not obliged to follow any Controlled Natural Language in their writing of\n",
      "requirements, we believe that language regularities are likely to emerge from\n",
      "this task, mainly due to the writers' experience. The issue is approached using\n",
      "natural language processing tools to identify sentences that do not comply with\n",
      "INCOSE rules. We further review these sentences to understand why the\n",
      "recommendations cannot (or should not) always be applied when specifying\n",
      "large-scale projects.\n",
      "\n",
      "==============================\n",
      "\n",
      "Digital Twin AI\n",
      "---------------\n",
      "Terahertz in-line digital holography of dragonfly hindwing: amplitude\n",
      "  and phase reconstruction at enhanced resolution by extrapolation , 2014\n",
      "['CH0']\n",
      "We report here on terahertz (THz) digital holography on a biological\n",
      "specimen. A continuous-wave (CW) THz in-line holographic setup was built based\n",
      "on a 2.52 THz CO2 pumped THz laser and a pyroelectric array detector. We\n",
      "introduced novel statistical method of obtaining true intensity values for the\n",
      "pyroelectric array detector's pixels. Absorption and phase-shifting images of a\n",
      "dragonfly's hind wing were reconstructed simultaneously from single in-line\n",
      "hologram. Furthermore, we applied phase retrieval routines to eliminate twin\n",
      "image and enhanced the resolution of the reconstructions by hologram\n",
      "extrapolation beyond the detector area. The finest observed features are 35\n",
      "{\\mu}m width cross veins.\n",
      "\n",
      "==============================\n",
      "\n",
      "Cyber Defense AI\n",
      "----------------\n",
      "Characterizing the Power of Moving Target Defense via Cyber Epidemic\n",
      "  Dynamics , 2014\n",
      "['UKG']\n",
      "Moving Target Defense (MTD) can enhance the resilience of cyber systems\n",
      "against attacks. Although there have been many MTD techniques, there is no\n",
      "systematic understanding and {\\em quantitative} characterization of the power\n",
      "of MTD. In this paper, we propose to use a cyber epidemic dynamics approach to\n",
      "characterize the power of MTD. We define and investigate two complementary\n",
      "measures that are applicable when the defender aims to deploy MTD to achieve a\n",
      "certain security goal. One measure emphasizes the maximum portion of time\n",
      "during which the system can afford to stay in an undesired configuration (or\n",
      "posture), without considering the cost of deploying MTD. The other measure\n",
      "emphasizes the minimum cost of deploying MTD, while accommodating that the\n",
      "system has to stay in an undesired configuration (or posture) for a given\n",
      "portion of time. Our analytic studies lead to algorithms for optimally\n",
      "deploying MTD.\n",
      "\n",
      "==============================\n",
      "\n",
      "Compliance AI\n",
      "-------------\n",
      "Compliance for reversible client/server interactions , 2014\n",
      "['ITC', 'ITG']\n",
      "In the setting of session behaviours, we study an extension of the concept of\n",
      "compliance when a disciplined form of backtracking is present. After adding\n",
      "checkpoints to the syntax of session behaviours, we formalise the operational\n",
      "semantics via a LTS, and define a natural notion of checkpoint compliance. We\n",
      "then obtain a co-inductive characterisation of such compliance relation, and an\n",
      "axiomatic presentation that is proved to be sound and complete. As a byproduct\n",
      "we get a decision procedure for the new compliance, being the axiomatic system\n",
      "algorithmic.\n",
      "\n",
      "==============================\n",
      "\n",
      "Knowledge Worker Aid AI\n",
      "-----------------------\n",
      "Hands-on experiments on intelligent behavior for mobile robots , 2014\n",
      "['DE4', 'DE3']\n",
      "In recent years, Artificial Intelligence techniques have emerged as useful\n",
      "tools for solving various engineering problems that were not possible or\n",
      "convenient to handle by traditional methods. AI has directly influenced many\n",
      "areas of computer science and becomes an important part of the engineering\n",
      "curriculum. However, determining the important topics for a single semester AI\n",
      "course is a nontrivial task, given the lack of a general methodology. AI\n",
      "concepts commonly overlap with many other disciplines involving a wide range of\n",
      "subjects, including applied approaches to more formal mathematical issues. This\n",
      "paper presents the use of a simple robotic platform to assist the learning of\n",
      "basic AI concepts. The study is guided through some simple experiments using\n",
      "autonomous mobile robots. The central algorithm is the Learning Automata. Using\n",
      "LA, each robot action is applied to an environment to be evaluated by means of\n",
      "a fitness value. The response of the environment is used by the automata to\n",
      "select its next action. This procedure holds until the goal task is reached.\n",
      "The proposal addresses the AI study by offering in LA a unifying context to\n",
      "draw together several of the topics of AI and motivating the students to learn\n",
      "by building some hands on laboratory exercises. The presented material has been\n",
      "successfully tested as AI teaching aide in the University of Guadalajara\n",
      "robotics group as it motivates students and increases enrolment and retention\n",
      "while educating better computer engineers.\n",
      "\n",
      "==============================\n",
      "\n",
      "Content Creation AI\n",
      "-------------------\n",
      "Hands-on experiments on intelligent behavior for mobile robots , 2014\n",
      "['DE4', 'DE3']\n",
      "In recent years, Artificial Intelligence techniques have emerged as useful\n",
      "tools for solving various engineering problems that were not possible or\n",
      "convenient to handle by traditional methods. AI has directly influenced many\n",
      "areas of computer science and becomes an important part of the engineering\n",
      "curriculum. However, determining the important topics for a single semester AI\n",
      "course is a nontrivial task, given the lack of a general methodology. AI\n",
      "concepts commonly overlap with many other disciplines involving a wide range of\n",
      "subjects, including applied approaches to more formal mathematical issues. This\n",
      "paper presents the use of a simple robotic platform to assist the learning of\n",
      "basic AI concepts. The study is guided through some simple experiments using\n",
      "autonomous mobile robots. The central algorithm is the Learning Automata. Using\n",
      "LA, each robot action is applied to an environment to be evaluated by means of\n",
      "a fitness value. The response of the environment is used by the automata to\n",
      "select its next action. This procedure holds until the goal task is reached.\n",
      "The proposal addresses the AI study by offering in LA a unifying context to\n",
      "draw together several of the topics of AI and motivating the students to learn\n",
      "by building some hands on laboratory exercises. The presented material has been\n",
      "successfully tested as AI teaching aide in the University of Guadalajara\n",
      "robotics group as it motivates students and increases enrolment and retention\n",
      "while educating better computer engineers.\n",
      "\n",
      "==============================\n",
      "\n",
      "Peer to Peer Networks AI\n",
      "------------------------\n",
      "Spatial Coordination Strategies in Future Ultra-Dense Wireless Networks , 2014\n",
      "['EL3', 'EL6']\n",
      "Ultra network densification is considered a major trend in the evolution of\n",
      "cellular networks, due to its ability to bring the network closer to the user\n",
      "side and reuse resources to the maximum extent. In this paper we explore\n",
      "spatial resources coordination as a key empowering technology for next\n",
      "generation (5G) ultra-dense networks. We propose an optimization framework for\n",
      "flexibly associating system users with a densely deployed network of access\n",
      "nodes, opting for the exploitation of densification and the control of overhead\n",
      "signaling. Combined with spatial precoding processing strategies, we design\n",
      "network resources management strategies reflecting various features, namely\n",
      "local vs global channel state information knowledge exploitation, centralized\n",
      "vs distributed implementation, and non-cooperative vs joint multi-node data\n",
      "processing. We apply these strategies to future UDN setups, and explore the\n",
      "impact of critical network parameters, that is, the densification levels of\n",
      "users and access nodes as well as the power budget constraints, to users\n",
      "performance. We demonstrate that spatial resources coordination is a key factor\n",
      "for capitalizing on the gains of ultra dense network deployments.\n",
      "\n",
      "==============================\n",
      "\n",
      "Emotion Recognition AI\n",
      "----------------------\n",
      "STIMONT: A core ontology for multimedia stimuli description , 2014\n",
      "['HR0']\n",
      "Affective multimedia documents such as images, sounds or videos elicit\n",
      "emotional responses in exposed human subjects. These stimuli are stored in\n",
      "affective multimedia databases and successfully used for a wide variety of\n",
      "research in psychology and neuroscience in areas related to attention and\n",
      "emotion processing. Although important all affective multimedia databases have\n",
      "numerous deficiencies which impair their applicability. These problems, which\n",
      "are brought forward in the paper, result in low recall and precision of\n",
      "multimedia stimuli retrieval which makes creating emotion elicitation\n",
      "procedures difficult and labor-intensive. To address these issues a new core\n",
      "ontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and\n",
      "extends W3C EmotionML format with an expressive and formal representation of\n",
      "affective concepts, high-level semantics, stimuli document metadata and the\n",
      "elicited physiology. The advantages of ontology in description of affective\n",
      "multimedia stimuli are demonstrated in a document retrieval experiment and\n",
      "compared against contemporary keyword-based querying methods. Also, a software\n",
      "tool Intelligent Stimulus Generator for retrieval of affective multimedia and\n",
      "construction of stimuli sequences is presented.\n",
      "\n",
      "==============================\n",
      "\n",
      "Image Recognition AI\n",
      "--------------------\n",
      "Hands-on experiments on intelligent behavior for mobile robots , 2014\n",
      "['DE4', 'DE3']\n",
      "In recent years, Artificial Intelligence techniques have emerged as useful\n",
      "tools for solving various engineering problems that were not possible or\n",
      "convenient to handle by traditional methods. AI has directly influenced many\n",
      "areas of computer science and becomes an important part of the engineering\n",
      "curriculum. However, determining the important topics for a single semester AI\n",
      "course is a nontrivial task, given the lack of a general methodology. AI\n",
      "concepts commonly overlap with many other disciplines involving a wide range of\n",
      "subjects, including applied approaches to more formal mathematical issues. This\n",
      "paper presents the use of a simple robotic platform to assist the learning of\n",
      "basic AI concepts. The study is guided through some simple experiments using\n",
      "autonomous mobile robots. The central algorithm is the Learning Automata. Using\n",
      "LA, each robot action is applied to an environment to be evaluated by means of\n",
      "a fitness value. The response of the environment is used by the automata to\n",
      "select its next action. This procedure holds until the goal task is reached.\n",
      "The proposal addresses the AI study by offering in LA a unifying context to\n",
      "draw together several of the topics of AI and motivating the students to learn\n",
      "by building some hands on laboratory exercises. The presented material has been\n",
      "successfully tested as AI teaching aide in the University of Guadalajara\n",
      "robotics group as it motivates students and increases enrolment and retention\n",
      "while educating better computer engineers.\n",
      "\n",
      "==============================\n",
      "\n",
      "Marketing Automation AI\n",
      "-----------------------\n",
      "Hands-on experiments on intelligent behavior for mobile robots , 2014\n",
      "['DE4', 'DE3']\n",
      "In recent years, Artificial Intelligence techniques have emerged as useful\n",
      "tools for solving various engineering problems that were not possible or\n",
      "convenient to handle by traditional methods. AI has directly influenced many\n",
      "areas of computer science and becomes an important part of the engineering\n",
      "curriculum. However, determining the important topics for a single semester AI\n",
      "course is a nontrivial task, given the lack of a general methodology. AI\n",
      "concepts commonly overlap with many other disciplines involving a wide range of\n",
      "subjects, including applied approaches to more formal mathematical issues. This\n",
      "paper presents the use of a simple robotic platform to assist the learning of\n",
      "basic AI concepts. The study is guided through some simple experiments using\n",
      "autonomous mobile robots. The central algorithm is the Learning Automata. Using\n",
      "LA, each robot action is applied to an environment to be evaluated by means of\n",
      "a fitness value. The response of the environment is used by the automata to\n",
      "select its next action. This procedure holds until the goal task is reached.\n",
      "The proposal addresses the AI study by offering in LA a unifying context to\n",
      "draw together several of the topics of AI and motivating the students to learn\n",
      "by building some hands on laboratory exercises. The presented material has been\n",
      "successfully tested as AI teaching aide in the University of Guadalajara\n",
      "robotics group as it motivates students and increases enrolment and retention\n",
      "while educating better computer engineers.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for term in [\"Natural Language Generation\",\n",
    "             \"Speech recognition\",\n",
    "             \"Virtual Agents\",\n",
    "             \"Machine Learning Platforms\",\n",
    "             \"AI-Optimized Hardware\",\n",
    "             \"Decision Management AI\",\n",
    "             \"Deep Learning Platforms\",\n",
    "             \"Biometrics AI\",\n",
    "             \"Robotic Processes Automation AI\",\n",
    "             \"Natural Language Processing\",\n",
    "             \"Digital Twin AI\",\n",
    "             \"Cyber Defense AI\",\n",
    "             \"Compliance AI\", \n",
    "             \"Knowledge Worker Aid AI\",\n",
    "             \"Content Creation AI\",\n",
    "             \"Peer to Peer Networks AI\",\n",
    "             \"Emotion Recognition AI\",\n",
    "             \"Image Recognition AI\",\n",
    "             \"Marketing Automation AI\"]:\n",
    "    print(term)\n",
    "    print(\"-\"*len(term))\n",
    "    top_doc, data, all_scores = generate_indicator(term)\n",
    "    print(top_doc['title_of_article'], \",\", top_doc['year_of_article'])\n",
    "    print(top_doc['terms_nuts1_article'])\n",
    "    print(top_doc['textBody_abstract_article'])\n",
    "    print(\"\\n==============================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
