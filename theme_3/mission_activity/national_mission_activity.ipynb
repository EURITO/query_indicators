{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "from query_indicators import generate_save_path\n",
    "from query_indicators import get_eu_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from collections import defaultdict\n",
    "from clio_lite import clio_search, clio_search_iter\n",
    "import io\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env variables\n",
    "mpl.rcParams['hatch.linewidth'] = 0.2\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['image.cmap'] = 'Pastel1'\n",
    "#os.environ['AWS_SHARED_CREDENTIALS_FILE'] = '/home/aidrissov/.aws/credentials'  # <--- Note: NOT nesta's AWS credentials\n",
    "#from os import path\n",
    "#print (\"File exists:\" + str(path.exists('/home/aidrissov/.aws/credentials')))\n",
    "#print (\"directory exists:\" + str(path.exists('/home/aidrissov/.aws/')))\n",
    "#TRY MANUAL CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some globals\n",
    "URL = \"https://search-eurito-prod-bbyn72q2rhx4ifj6h5dom43uhy.eu-west-1.es.amazonaws.com/\"\n",
    "INDEX = \"arxiv_v0\" \n",
    "FIELDS = ['terms_tokens_entity', 'textBody_abstract_article']\n",
    "EU_COUNTRIES = get_eu_countries()\n",
    "COLORS = plt.get_cmap('Set2').colors\n",
    "COLOR_MAP = 'Pastel1'\n",
    "S3 = boto3.resource('s3')\n",
    "SAVE_PATH = generate_save_path()  # EURITO collaborators: this is generated assuming you have stuck to the convention 'theme_x/something/something_else.ipynb'\n",
    "BUCKET = 'eurito-indicators'  # EURITO collaborators: please don't change this\n",
    "SAVE_RESULTS = True  # Set this to \"False\" when you want to view figures inline. When \"True\", results will be saved to S3.\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "    plt.ioff()  # <--- for turning off visible figs\n",
    "else:\n",
    "    plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_search(query, max_query_terms, yr0=2014, yr1=2019, countries=EU_COUNTRIES, window=1):\n",
    "    \"\"\"\n",
    "    Retrieve count and score data for a given basic clio search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Seed query for clio.\n",
    "        max_query_terms (list): Triple of max_query_terms (low, middle, high) to use from the initial query.\n",
    "        yr0 (int): Start year in range to use in filter.\n",
    "        yr1 (int): Final year in range to use in filter.\n",
    "        countries (list): A list of countries to filter (default to all EU).\n",
    "        window (int): The number of years to consider in between time windows. Note that changing this will lead to double-counting.\n",
    "    Returns:\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each country]}\n",
    "        all_scores (dict): {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "    \"\"\"\n",
    "    top_doc = None\n",
    "    _data = defaultdict(lambda: defaultdict(dict))  # {max_query_terms --> {year --> {country --> score} } }\n",
    "    all_scores = defaultdict(lambda: defaultdict(list))  # {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "    for n in max_query_terms:\n",
    "        # Set the order of the countries\n",
    "        for ctry in EU_COUNTRIES:\n",
    "            _data[n][ctry]\n",
    "            all_scores[n][ctry]\n",
    "        # Iterate over years\n",
    "        for yr in range(yr0, yr1+1):\n",
    "            # Set default values for countries\n",
    "            for ctry in EU_COUNTRIES:\n",
    "                _data[n][ctry][yr] = 0            \n",
    "            # Iterate over docs\n",
    "            filters = [{\"range\":{\"year_of_article\":{\"gte\":yr, \"lt\":yr+window}}}]\n",
    "            for doc in clio_search_iter(url=URL, index=INDEX, query=query, fields=FIELDS,\n",
    "                                        max_query_terms=n, post_filters=filters, chunksize=5000):\n",
    "                if '_score' not in doc or doc['terms_countries_article'] is None:\n",
    "                    continue\n",
    "                score = doc['_score']\n",
    "                for ctry in filter(lambda x: x in countries, doc['terms_countries_article']):\n",
    "                    if top_doc is None:\n",
    "                        top_doc = doc                \n",
    "                    all_scores[n][ctry].append(score)\n",
    "                    _data[n][ctry][yr] += score\n",
    "    # Reformat data as {max_query_terms --> [{year --> score} for each country in order]}\n",
    "    data = {}\n",
    "    for n, ctry_data in _data.items():\n",
    "        data[n] = []\n",
    "        for ctry, yr_data in ctry_data.items():\n",
    "            data[n].append(yr_data)\n",
    "    return top_doc, data, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator calculations\n",
    "\n",
    "Each of these functions is assumed to take the form\n",
    "\n",
    "```python\n",
    "def _an_indicator_calulation(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    A function calculating an indicator.\n",
    "    \n",
    "    Args:\n",
    "        data (list): Rows of data\n",
    "        year (int): A year to consider, if applicable.\n",
    "        _max (int): Divide by this to normalise your results. This is automatically applied in :obj:`make_activity_plot`\n",
    "    Returns:\n",
    "        result (list) A list of indicators to plot. The length of the list is assumed to be equal to the number of countries.\n",
    "    \"\"\"\n",
    "    # Calculate something\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _total_activity_by_country(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Sum of relevance scores, by year (if specified) or in total.\n",
    "    \"\"\"    \n",
    "    if year is None:        \n",
    "        scores = [sum(row.values())/_max for row in data]\n",
    "    else:\n",
    "        scores = [row[year]/_max for row in data]\n",
    "    return scores\n",
    "      \n",
    "\n",
    "def _average_activity_by_country(data, year=None, _max=1):    \n",
    "    \"\"\"\n",
    "    Indicator: Mean relevance score. This function is basically a lambda, since it assumes the average has already been calculated.\n",
    "    \"\"\"        \n",
    "    return [row/_max for row in data]\n",
    "    \n",
    "    \n",
    "def _corrected_average_activity_by_country(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Mean relevance score minus it's (very) approximate Poisson error.\n",
    "    \"\"\"    \n",
    "    return [(row - np.sqrt(row))/_max for row in data]\n",
    "    \n",
    "\n",
    "def _linear_coeffs(years, scores, _max):\n",
    "    \"\"\"Calculates linear coefficients for scores wrt years\"\"\"\n",
    "    return [np.polyfit(_scores, _years, 1)[0]/_max\n",
    "            if all(v > 0 for v in _scores) else 0\n",
    "            for _years, _scores in zip(years, scores)]    \n",
    "    \n",
    "\n",
    "def _trajectory(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Linear coefficient of total relevance score wrt year\n",
    "    \"\"\"\n",
    "    years = [list(row.keys()) for row in data]\n",
    "    scores = [list(row.values()) for row in data]\n",
    "    return _linear_coeffs(years, scores, _max)\n",
    "\n",
    "\n",
    "def _corrected_trajectory(data, year=None, _max=1):\n",
    "    \"\"\"\n",
    "    Indicator: Linear coefficient of upper and lower limits of relevance score wrt year\n",
    "    \"\"\" \n",
    "    # Reformulate the data in terms of upper and lower bounds\n",
    "    years, scores = [], []\n",
    "    for row in data:\n",
    "        _years, _scores = [], []\n",
    "        for k, v in row.items():\n",
    "            _years += [k,k]\n",
    "            _scores += [v - np.sqrt(v), v + np.sqrt(v)]  # Estimate upper and lower limits with very approximate Poisson errors\n",
    "        years.append(_years)\n",
    "        scores.append(_scores)\n",
    "    return _linear_coeffs(years, scores, _max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Sorter:\n",
    "    def __init__(self, values, topn=None):\n",
    "        if topn is None:\n",
    "            topn = len(values)\n",
    "        self.indices = list(np.argsort(values))[-topn:]  # Argsort is ascending, so -ve indexing to pick up topn\n",
    "    def sort(self, x):\n",
    "        \"\"\"Sort list x by indices\"\"\"\n",
    "        return [x[i] for i in self.indices]\n",
    "\n",
    "\n",
    "def _s3_savefig(query, fig_name, extension='png'):\n",
    "    \"\"\"Save the figure to s3. The figure is grabbed from the global scope.\"\"\"\n",
    "    if not SAVE_RESULTS:\n",
    "        return    \n",
    "    outname = (f'figures/{SAVE_PATH}/'\n",
    "               f'{query.replace(\" \",\"_\").lower()}'\n",
    "               f'/{fig_name.replace(\" \",\"_\").lower()}'\n",
    "               f'.{extension}')\n",
    "    with io.BytesIO() as f:\n",
    "        plt.savefig(f, bbox_inches='tight', format=extension, pad_inches=0)\n",
    "        obj = S3.Object(BUCKET, outname)\n",
    "        f.seek(0)\n",
    "        obj.put(Body=f)\n",
    "\n",
    "        \n",
    "def _s3_savetable(data, key, index, object_path, transformer=lambda x: x):\n",
    "    \"\"\"Upload the table to s3\"\"\"\n",
    "    if not SAVE_RESULTS:\n",
    "        return\n",
    "    df = pd.DataFrame(transformer(data[key]), index=index)\n",
    "    if len(df.columns) == 1:\n",
    "        df.columns = ['value']\n",
    "    df = df / df.max().max()\n",
    "    table_data = df.to_csv().encode()\n",
    "    obj = S3.Object(BUCKET, os.path.join(f'tables/{SAVE_PATH}', object_path))\n",
    "    obj.put(Body=table_data)\n",
    "\n",
    "        \n",
    "def make_activity_plot(f, data, countries, max_query_terms, query, \n",
    "                       year=None, label=None, x_padding=0.5, y_padding=0.05, xlabel_fontsize=14):\n",
    "    \"\"\"\n",
    "    Make a query and generate indicators by country, saving the plots to S3 and saving the rawest data\n",
    "    to tables on S3.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        f: An indicator function, as described in the 'Indicator calculations' section.\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each country]}\n",
    "        countries (list): A list of EU ISO-2 codes        \n",
    "        max_query_terms (list): Triple of max_query_terms for clio, corresponding to low, middle and high values of \n",
    "                                max_query_terms to test robustness of the query.\n",
    "        query (str): query used to generate this data.\n",
    "        year (int): Year to generate the indicator for (if applicable).\n",
    "        label (str): label for annotating the plot.\n",
    "        {x,y}_padding (float): Aesthetic padding around the extreme limits of the {x,y} axis.\n",
    "        xlabel_fontsize (int): Fontsize of the x labels (country ISO-2 codes).\n",
    "    \"\"\"    \n",
    "    # Calculate the indicator for each value of n, then recalculate the normalised indicator\n",
    "    _, middle, _ = (f(data[n], year=year) for n in max_query_terms)\n",
    "    low, middle, high = (f(data[n], year=year, _max=max(middle)) for n in max_query_terms)\n",
    "    indicator = [np.median([a, b, c]) for a, b, c in zip(low, middle, high)]    \n",
    "\n",
    "    # Sort all data by indicator value\n",
    "    s = _Sorter(indicator)\n",
    "    countries = s.sort(countries)\n",
    "    low = s.sort(low)\n",
    "    middle = s.sort(middle)\n",
    "    high =  s.sort(high)\n",
    "    indicator = s.sort(indicator)\n",
    "\n",
    "    # Make the scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))    \n",
    "    make_error_boxes(ax, low, middle, high)  # Draw the bounding box\n",
    "    ax.scatter(countries, indicator,  s=0, marker='o', color='black')  # Draw the centre mark\n",
    "    ax.set_title(f'{label}\\nQuery: \"{query}\"')\n",
    "    ax.set_ylabel(label)\n",
    "\n",
    "    # Set limits and formulate \n",
    "    y0 = min(low+middle+high)    \n",
    "    y1 = max(low+middle+high)\n",
    "    if -y1*y_padding < y0:\n",
    "        y0 = -y1*y_padding\n",
    "    else:  # In case of negative values\n",
    "        y0 = y0 - np.abs(y0*y_padding)\n",
    "    ax.set_ylim(y0, y1*(1+y_padding))\n",
    "    ax.set_xlim(-x_padding, len(countries)-x_padding)\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(xlabel_fontsize)\n",
    "    \n",
    "    # Save to s3 & return\n",
    "    _s3_savefig(query, label)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def make_error_boxes(ax, low, middle, high, facecolor='r',\n",
    "                     edgecolor='None', alpha=0.5):\n",
    "    \"\"\"\n",
    "    Generate outer rectangles based on three values, and draw a horizontal line through the middle of the rectangle.\n",
    "    No assumption is made on the order of values, so don't worry if they're not properly ordered.\n",
    "        \n",
    "    Args:\n",
    "        ax (matplotlib.axis): An axis to add patches to.\n",
    "        {low, middle, high} (list): Three concurrent lists of values from which to calculate the rectangle limits.\n",
    "        {facecolor, edgecolor} (str): The {face,edge} colour of the rectangles.\n",
    "        alpha (float): The alpha of the rectangles.\n",
    "    \"\"\"\n",
    "    # Generate the rectangle\n",
    "    errorboxes = []\n",
    "    middlelines = []\n",
    "    for x, ys in enumerate(zip(low, middle, high)):        \n",
    "        rect = Rectangle((x - 0.45, min(ys)), 0.9, max(ys) - min(ys))\n",
    "        line = Rectangle((x - 0.45, np.median(ys)), 0.9, 0)\n",
    "        errorboxes.append(rect)\n",
    "        middlelines.append(line)\n",
    "\n",
    "    # Create patch collection with specified colour/alpha\n",
    "    pc = PatchCollection(errorboxes, facecolor=facecolor, alpha=alpha, edgecolor=edgecolor, hatch='/')\n",
    "    lc = PatchCollection(middlelines, facecolor='black', alpha=0.9, edgecolor='black')\n",
    "\n",
    "    # Add collection to axes\n",
    "    ax.add_collection(pc)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "\n",
    "def stacked_scores(all_scores, query, topn=8,\n",
    "                   low_bins=[10**i for i in np.arange(0, 1.1, 0.025)],\n",
    "                   high_bins=[10**i for i in np.arange(1.1, 2.5, 0.05)],\n",
    "                   x_scale='log', label='Relevance score breakdown', \n",
    "                   xlabel='Relevance score', ylabel='Number of relevant documents',\n",
    "                   legend_fontsize='small', legend_cols=2):\n",
    "    \"\"\"\n",
    "    Create stacked histogram of document scores by country. Two sets of bins are used, \n",
    "    in order to have a more legible binning scale.\n",
    "    \n",
    "    Args:\n",
    "        all_scores (dict): {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "        query (str): query used to generate this data.\n",
    "        low_bins (list): List of initial bin edges.\n",
    "        high_bins (list): List of supplementary bin edges. These could have a different spacing scheme to the lower bin edges.\n",
    "        x_scale (str): Argument for `ax.set_xscale`.\n",
    "        label (str): label for annotating the plot.\n",
    "        {x,y}_label (str): Argument for `ax.set_{x,y}label`.\n",
    "        legend_fontsize (str): Argument for legend fontsize.\n",
    "        legend_cols (str): Argument for legend ncol.        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort countries and scores by the sum of scores by country\n",
    "    countries = list(all_scores.keys())\n",
    "    scores = list(all_scores.values())    \n",
    "    s = _Sorter([sum(v) for v in scores], topn=topn)\n",
    "    scores = s.sort(scores)\n",
    "    countries = s.sort(countries)\n",
    "\n",
    "    # Plot the stacked scores\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.set_cmap(COLOR_MAP)\n",
    "    ax.hist(scores, bins=low_bins+high_bins, stacked=True,\n",
    "            label=countries, color=COLORS[:len(scores)])\n",
    "    \n",
    "    # Prettify the plot\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend(fontsize=legend_fontsize, ncol=legend_cols)\n",
    "    ax.set_xlim(low_bins[0], None)\n",
    "    ax.set_xscale(x_scale)\n",
    "    ax.set_title(f'{label}\\nQuery: \"{query}\"')\n",
    "    \n",
    "    # Save to s3\n",
    "    _s3_savefig(query, label)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indicator(q, max_query_terms=[7, 10, 13], countries=EU_COUNTRIES, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Make a query and generate indicators by country, saving the plots to S3 and saving the rawest data\n",
    "    to tables on S3.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        q (str): The query to Elasticsearch\n",
    "        max_query_terms (list): Triple of max_query_terms for clio, corresponding to low, middle and high values of \n",
    "                                max_query_terms to test robustness of the query.\n",
    "        countries (list): A list of EU ISO-2 codes\n",
    "    Returns:\n",
    "        top_doc (dict): The highest ranking document from the search.\n",
    "        data (dict): {max_query_terms --> [{year --> sum_score} for each country]}\n",
    "        all_scores (dict): {max_query_terms --> {country --> [score for doc in docs] } }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make the search and retrieve scores by country, and the highest ranking doc\n",
    "    example_doc, data, all_scores = make_search(q, max_query_terms=max_query_terms, countries=countries, *args, **kwargs)\n",
    "\n",
    "    # Reformat the scores to calculate the average\n",
    "    avg_scores = defaultdict(list)\n",
    "    for ctry in countries:\n",
    "        for n, _scores in all_scores.items():\n",
    "            mean = np.mean(_scores[ctry]) if len(_scores[ctry]) > 0 else 0\n",
    "            avg_scores[n].append(mean)\n",
    "    \n",
    "    plot_kwargs = dict(countries=countries, max_query_terms=max_query_terms, query=q)\n",
    "    # Calculate loads of indicators and save the plots\n",
    "    _ = make_activity_plot(_total_activity_by_country, data, label='Total relevance score', **plot_kwargs)\n",
    "    _ = make_activity_plot(_average_activity_by_country, avg_scores, label='Average relevance', **plot_kwargs)\n",
    "    _ = make_activity_plot(_corrected_average_activity_by_country, avg_scores, label='Corrected average relevance',  **plot_kwargs)\n",
    "    _ = make_activity_plot(_trajectory, data, label='Trajectory', **plot_kwargs)\n",
    "    _ = make_activity_plot(_corrected_trajectory, data, label='Corrected trajectory', **plot_kwargs)\n",
    "    _ = stacked_scores(all_scores[max_query_terms[1]], query=q)\n",
    "    \n",
    "    # Save the basic raw data as tables. Note: not as rich as the plotted data.\n",
    "    _q = q.replace(\" \",\"_\").lower()\n",
    "    _s3_savetable(data, max_query_terms[1], index=countries, object_path=f'{_q}/LMA.csv')\n",
    "    _s3_savetable(avg_scores, max_query_terms[1], index=countries, object_path=f'{_q}/avg_LMA.csv')\n",
    "    \n",
    "    plt.close('all')  # Clean up the memory cache (unbelievable that matplotlib doesn't do this)\n",
    "    return example_doc, data, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptation to climate change, including societal transformation\n",
      "---------------------------------------------------------------\n",
      "Validity of altmetrics data for measuring societal impact: A study using\n",
      "  data from Altmetric and F1000Prime , 2014\n",
      "['DE']\n",
      "Can altmetric data be validly used for the measurement of societal impact?\n",
      "The current study seeks to answer this question with a comprehensive dataset\n",
      "(about 100,000 records) from very disparate sources (F1000, Altmetric, and an\n",
      "in-house database based on Web of Science). In the F1000 peer review system,\n",
      "experts attach particular tags to scientific papers which indicate whether a\n",
      "paper could be of interest for science or rather for other segments of society.\n",
      "The results show that papers with the tag \"good for teaching\" do achieve higher\n",
      "altmetric counts than papers without this tag - if the quality of the papers is\n",
      "controlled. At the same time, a higher citation count is shown especially by\n",
      "papers with a tag that is specifically scientifically oriented (\"new finding\").\n",
      "The findings indicate that papers tailored for a readership outside the area of\n",
      "research should lead to societal impact. If altmetric data is to be used for\n",
      "the measurement of societal impact, the question arises of its normalization.\n",
      "In bibliometrics, citations are normalized for the papers' subject area and\n",
      "publication year. This study has taken a second analytic step involving a\n",
      "possible normalization of altmetric data. As the results show there are\n",
      "particular scientific topics which are of especial interest for a wide\n",
      "audience. Since these more or less interesting topics are not completely\n",
      "reflected in Thomson Reuters' journal sets, a normalization of altmetric data\n",
      "should not be based on the level of subject categories, but on the level of\n",
      "topics.\n",
      "\n",
      "==============================\n",
      "\n",
      "Cancer\n",
      "------\n",
      "A Statistical Approach to Identifying Significant Transgenerational\n",
      "  Methylation Changes , 2014\n",
      "['GB', 'US', 'CH', 'CA', 'IE']\n",
      "Epigenetic aberrations have profound effects on phenotypic output. Genome\n",
      "wide methylation alterations are inheritable to pass down the aberrations\n",
      "through multiple generations. We developed a statistical method, Genome-wide\n",
      "Identification of Significant Methylation Alteration, GISAIM, to study the\n",
      "significant transgenerational methylation changes. GISAIM finds the significant\n",
      "methylation aberrations that are inherited through multiple generations. In a\n",
      "concrete biological study, we investigated whether exposing pregnant rats (F0)\n",
      "to a high fat (HF) diet throughout pregnancy or ethinyl estradiol\n",
      "(EE2)-supplemented diet during gestation days 14 20 affects carcinogen-induced\n",
      "mammary cancer risk in daughters (F1), granddaughters (F2) and\n",
      "great-granddaughters (F3). Mammary tumorigenesis was higher in daughters and\n",
      "granddaughters of HF rat dams, and in daughters, granddaughters and\n",
      "great-granddaughters of EE2 rat dams. Outcross experiments showed that\n",
      "increased mammary cancer risk was transmitted to HF granddaughters equally\n",
      "through the female or male germlines, but is only transmitted to EE2\n",
      "granddaughters through the female germline. Transgenerational effect on mammary\n",
      "cancer risk was associated with increased expression of DNA methyltransferases,\n",
      "and across all three EE2 generations hypo or hyper methylation of the same 375\n",
      "gene promoter regions in their mammary glands. Our study shows that maternal\n",
      "dietary estrogenic exposures during pregnancy can increase breast cancer risk\n",
      "in multiple generations of offspring, and the increase in risk may be inherited\n",
      "through non-genetic means, possibly involving DNA methylation.\n",
      "\n",
      "==============================\n",
      "\n",
      "Climate-neutral and smart cities\n",
      "--------------------------------\n",
      "Software-Defined and Virtualized Future Mobile and Wireless Networks: A\n",
      "  Survey , 2014\n",
      "['CN', 'CH', 'GR']\n",
      "With the proliferation of mobile demands and increasingly multifarious\n",
      "services and applications, mobile Internet has been an irreversible trend.\n",
      "Unfortunately, the current mobile and wireless network (MWN) faces a series of\n",
      "pressing challenges caused by the inherent design. In this paper, we extend two\n",
      "latest and promising innovations of Internet, software-defined networking and\n",
      "network virtualization, to mobile and wireless scenarios. We first describe the\n",
      "challenges and expectations of MWN, and analyze the opportunities provided by\n",
      "the software-defined wireless network (SDWN) and wireless network\n",
      "virtualization (WNV). Then, this paper focuses on SDWN and WNV by presenting\n",
      "the main ideas, advantages, ongoing researches and key technologies, and open\n",
      "issues respectively. Moreover, we interpret that these two technologies highly\n",
      "complement each other, and further investigate efficient joint design between\n",
      "them. This paper confirms that SDWN and WNV may efficiently address the crucial\n",
      "challenges of MWN and significantly benefit the future mobile and wireless\n",
      "network.\n",
      "\n",
      "==============================\n",
      "\n",
      "Soil health and food\n",
      "--------------------\n",
      "GREEND: An Energy Consumption Dataset of Households in Italy and Austria , 2014\n",
      "['AT']\n",
      "Home energy management systems can be used to monitor and optimize\n",
      "consumption and local production from renewable energy. To assess solutions\n",
      "before their deployment, researchers and designers of those systems demand for\n",
      "energy consumption datasets. In this paper, we present the GREEND dataset,\n",
      "containing detailed power usage information obtained through a measurement\n",
      "campaign in households in Austria and Italy. We provide a description of\n",
      "consumption scenarios and discuss design choices for the sensing\n",
      "infrastructure. Finally, we benchmark the dataset with state-of-the-art\n",
      "techniques in load disaggregation, occupancy detection and appliance usage\n",
      "mining.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for term in [\"Adaptation to climate change, including societal transformation\",\n",
    "             \"Cancer\",\n",
    "             \"Climate-neutral and smart cities\",\n",
    "             \"Soil health and food\"]:\n",
    "    print(term)\n",
    "    print(\"-\"*len(term))\n",
    "    top_doc, data, all_scores = generate_indicator(term)\n",
    "    print(top_doc['title_of_article'], \",\", top_doc['year_of_article'])\n",
    "    print(top_doc['terms_countries_article'])\n",
    "    print(top_doc['textBody_abstract_article'])\n",
    "    print(\"\\n==============================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
